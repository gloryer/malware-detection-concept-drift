{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ba8a66-f24e-4226-aeec-434598522d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d7d7a-cee7-413e-87fc-392f962704c1",
   "metadata": {},
   "source": [
    "# Useful functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf01003-c533-4b6f-a4f3-a3e2bb31f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_label(df, y, label):\n",
    "    # Splits the dataset into two parts based on the specified label.\n",
    "    # Returns the subset of the dataframe and labels matching the label,\n",
    "    # and the subset of the dataframe and labels not matching the label.\n",
    "     \n",
    "    output_df = df[np.where(y == label)[0]]\n",
    "    \n",
    "    res_df =  df[np.where(y != label)[0]]\n",
    "    \n",
    "    output_y = y[np.where(y == label)[0]]\n",
    "    res_y = y[np.where(y != label)[0]]\n",
    "    \n",
    "    return  output_df, output_y, res_df, res_y\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def obtain_labels(df, label_path):\n",
    "    # Maps feature data from the dataframe to their corresponding labels from a CSV file.\n",
    "    # Returns arrays of features and their corresponding labels.\n",
    "    \n",
    "    labels = pd.read_csv(label_path, header=0)\n",
    "    \n",
    "    y = []\n",
    "    x = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        #print(row[\"asm_id\"].split(\".\")[0])\n",
    "        hash_id = row[\"asm_id\"].split(\".\")[0]\n",
    "        if hash_id in labels['asm_id'].values: \n",
    "            row = row.drop(\"asm_id\")\n",
    "            x.append(row)\n",
    "            y.append(labels[labels[\"asm_id\"] == hash_id][\"Class\"])\n",
    "            \n",
    "\n",
    "    \n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "\n",
    "def change_attack_label(x):\n",
    "    # Changes the attack label to 1.\n",
    "    # Returns a list with a single element [1.].\n",
    "    label = [1.]\n",
    "    return label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(x, y, shuffle=True, batch_size=128, epochs = 60):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y)).repeat(epochs)\n",
    "    #ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(x))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417cc5e2-02e5-41cf-a514-7e3942bc1de5",
   "metadata": {},
   "source": [
    "# Load malware data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07e47c46-d416-4170-9471-8bfa3058d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10864, 965)\n"
     ]
    }
   ],
   "source": [
    "path = \"data/content_features/Big15_2/FeatureCategories/train/asm_full.csv\"\n",
    "malware_x = pd.read_csv(path, header=0)\n",
    "print(malware_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de0786d6-87bb-4f26-ad20-0df953a0802b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10708, 964)\n",
      "(10708, 1)\n"
     ]
    }
   ],
   "source": [
    "label_path = \"data/labels/new_gmm_labels_id.csv\"\n",
    "\n",
    "malware_x, malware_y = obtain_labels(malware_x, label_path)\n",
    "print(malware_x.shape)\n",
    "print(malware_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "203fb423-9211-4698-b750-2ee50383574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data using StandardScaler\n",
    "malware_x= StandardScaler().fit_transform(malware_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d99903-2e10-49a1-bb9b-5d6f84844a9f",
   "metadata": {},
   "source": [
    "# Load normal data binarycorp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f1e9e6-364a-4ab8-ac5e-63ad4f420f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6396, 965)\n"
     ]
    }
   ],
   "source": [
    "path = \"data/content_features/Binary_corp/FeatureCategories/train/asm_full.csv\"\n",
    "source_normal_x = pd.read_csv(path, header=0)\n",
    "print(source_normal_x .shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "265eec0f-3d6b-44ef-b788-abda54da941a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asm_id</th>\n",
       "      <th>asm_md_filesize</th>\n",
       "      <th>asm_md_loc</th>\n",
       "      <th>asm_symb_Star</th>\n",
       "      <th>asm_symb_Dash</th>\n",
       "      <th>asm_symb_Plus</th>\n",
       "      <th>asm_symb_Bracket_Open</th>\n",
       "      <th>asm_symb_Bracket_Close</th>\n",
       "      <th>asm_symb_AtSign</th>\n",
       "      <th>asm_symb_Question</th>\n",
       "      <th>...</th>\n",
       "      <th>GetEnhMetaFilePaletteEntries</th>\n",
       "      <th>IsBadCodePtr</th>\n",
       "      <th>OpenMutexA</th>\n",
       "      <th>__vbaStrI4</th>\n",
       "      <th>__vbaVarCopy</th>\n",
       "      <th>CopyEnhMetaFileA</th>\n",
       "      <th>LocalReAlloc</th>\n",
       "      <th>srand</th>\n",
       "      <th>__vbaAryCopy</th>\n",
       "      <th>CreateDCA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2bwm-git-2bwm-O0-f9579e061f6e200bc50fdae0d8f2a...</td>\n",
       "      <td>1905909</td>\n",
       "      <td>26506</td>\n",
       "      <td>114</td>\n",
       "      <td>2144</td>\n",
       "      <td>2861</td>\n",
       "      <td>2959</td>\n",
       "      <td>2960</td>\n",
       "      <td>193</td>\n",
       "      <td>1144</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2bwm-git-2bwm-O1-f912e10f4da05c16416b15fc91ceb...</td>\n",
       "      <td>1362048</td>\n",
       "      <td>18570</td>\n",
       "      <td>123</td>\n",
       "      <td>222</td>\n",
       "      <td>2786</td>\n",
       "      <td>1492</td>\n",
       "      <td>1493</td>\n",
       "      <td>111</td>\n",
       "      <td>1199</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2bwm-git-2bwm-O2-78b862c439faaf310297881d8cb2e...</td>\n",
       "      <td>1570178</td>\n",
       "      <td>21379</td>\n",
       "      <td>127</td>\n",
       "      <td>444</td>\n",
       "      <td>2910</td>\n",
       "      <td>1565</td>\n",
       "      <td>1569</td>\n",
       "      <td>171</td>\n",
       "      <td>1193</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2bwm-git-2bwm-O3-24b389b88252b33cdb108afec106b...</td>\n",
       "      <td>1640222</td>\n",
       "      <td>22268</td>\n",
       "      <td>137</td>\n",
       "      <td>448</td>\n",
       "      <td>3193</td>\n",
       "      <td>1738</td>\n",
       "      <td>1741</td>\n",
       "      <td>169</td>\n",
       "      <td>1193</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2bwm-git-2bwm-Os-2376158d9aecb465e30ec25af58df...</td>\n",
       "      <td>1340232</td>\n",
       "      <td>18202</td>\n",
       "      <td>121</td>\n",
       "      <td>211</td>\n",
       "      <td>2583</td>\n",
       "      <td>1315</td>\n",
       "      <td>1310</td>\n",
       "      <td>99</td>\n",
       "      <td>1195</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 965 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              asm_id  asm_md_filesize  \\\n",
       "0  2bwm-git-2bwm-O0-f9579e061f6e200bc50fdae0d8f2a...          1905909   \n",
       "1  2bwm-git-2bwm-O1-f912e10f4da05c16416b15fc91ceb...          1362048   \n",
       "2  2bwm-git-2bwm-O2-78b862c439faaf310297881d8cb2e...          1570178   \n",
       "3  2bwm-git-2bwm-O3-24b389b88252b33cdb108afec106b...          1640222   \n",
       "4  2bwm-git-2bwm-Os-2376158d9aecb465e30ec25af58df...          1340232   \n",
       "\n",
       "   asm_md_loc  asm_symb_Star  asm_symb_Dash  asm_symb_Plus  \\\n",
       "0       26506            114           2144           2861   \n",
       "1       18570            123            222           2786   \n",
       "2       21379            127            444           2910   \n",
       "3       22268            137            448           3193   \n",
       "4       18202            121            211           2583   \n",
       "\n",
       "   asm_symb_Bracket_Open  asm_symb_Bracket_Close  asm_symb_AtSign  \\\n",
       "0                   2959                    2960              193   \n",
       "1                   1492                    1493              111   \n",
       "2                   1565                    1569              171   \n",
       "3                   1738                    1741              169   \n",
       "4                   1315                    1310               99   \n",
       "\n",
       "   asm_symb_Question  ...  GetEnhMetaFilePaletteEntries  IsBadCodePtr  \\\n",
       "0               1144  ...                             0             0   \n",
       "1               1199  ...                             0             0   \n",
       "2               1193  ...                             0             0   \n",
       "3               1193  ...                             0             0   \n",
       "4               1195  ...                             0             0   \n",
       "\n",
       "   OpenMutexA  __vbaStrI4  __vbaVarCopy  CopyEnhMetaFileA  LocalReAlloc  \\\n",
       "0           0           0             0                 0             0   \n",
       "1           0           0             0                 0             0   \n",
       "2           0           0             0                 0             0   \n",
       "3           0           0             0                 0             0   \n",
       "4           0           0             0                 0             0   \n",
       "\n",
       "   srand  __vbaAryCopy  CreateDCA  \n",
       "0      0             0          0  \n",
       "1      0             0          0  \n",
       "2      0             0          0  \n",
       "3      0             0          0  \n",
       "4      0             0          0  \n",
       "\n",
       "[5 rows x 965 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_normal_x.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6283fc6-fbf6-48c2-a2cd-48c79a842ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6396, 1)\n"
     ]
    }
   ],
   "source": [
    "#create the labels for the normal data\n",
    "source_normal_y = np.zeros((source_normal_x.shape[0],1))\n",
    "print(source_normal_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ec8676-5201-4169-8983-4b2e2779c720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6396, 964)\n"
     ]
    }
   ],
   "source": [
    "# Drop the asm_id column\n",
    "source_normal_x = source_normal_x.drop(\"asm_id\", axis = 1)\n",
    "print(source_normal_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff9c18b6-7c27-43b6-bfe4-22bc6c3612c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data using StandardScaler\n",
    "source_normal_x= StandardScaler().fit_transform(source_normal_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63be8a6-80bd-4d10-ab47-10ee619bb3b5",
   "metadata": {},
   "source": [
    "# Load normal data windows-PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a539ddd8-6b23-48f0-b8ae-cf2dc4307fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989, 965)\n"
     ]
    }
   ],
   "source": [
    "path = \"data/content_features/Windows_PE/FeatureCategories/train/asm_full.csv\"\n",
    "target_normal_x = pd.read_csv(path, header=0)\n",
    "print(target_normal_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd7846c3-5591-4581-9da1-1696f928e43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asm_id</th>\n",
       "      <th>asm_md_filesize</th>\n",
       "      <th>asm_md_loc</th>\n",
       "      <th>asm_symb_Star</th>\n",
       "      <th>asm_symb_Dash</th>\n",
       "      <th>asm_symb_Plus</th>\n",
       "      <th>asm_symb_Bracket_Open</th>\n",
       "      <th>asm_symb_Bracket_Close</th>\n",
       "      <th>asm_symb_AtSign</th>\n",
       "      <th>asm_symb_Question</th>\n",
       "      <th>...</th>\n",
       "      <th>GetEnhMetaFilePaletteEntries</th>\n",
       "      <th>IsBadCodePtr</th>\n",
       "      <th>OpenMutexA</th>\n",
       "      <th>__vbaStrI4</th>\n",
       "      <th>__vbaVarCopy</th>\n",
       "      <th>CopyEnhMetaFileA</th>\n",
       "      <th>LocalReAlloc</th>\n",
       "      <th>srand</th>\n",
       "      <th>__vbaAryCopy</th>\n",
       "      <th>CreateDCA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64BitMAPIBroker_exe.asm</td>\n",
       "      <td>9316026</td>\n",
       "      <td>127559</td>\n",
       "      <td>408</td>\n",
       "      <td>4727</td>\n",
       "      <td>19269</td>\n",
       "      <td>14054</td>\n",
       "      <td>14066</td>\n",
       "      <td>1514</td>\n",
       "      <td>19061</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7z_exe.asm</td>\n",
       "      <td>14349207</td>\n",
       "      <td>195374</td>\n",
       "      <td>733</td>\n",
       "      <td>6564</td>\n",
       "      <td>41002</td>\n",
       "      <td>24276</td>\n",
       "      <td>24272</td>\n",
       "      <td>258</td>\n",
       "      <td>13061</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADelRCP_exe.asm</td>\n",
       "      <td>3930867</td>\n",
       "      <td>59845</td>\n",
       "      <td>193</td>\n",
       "      <td>5793</td>\n",
       "      <td>2723</td>\n",
       "      <td>7484</td>\n",
       "      <td>7485</td>\n",
       "      <td>626</td>\n",
       "      <td>8642</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARPPRODUCTICON_exe.asm</td>\n",
       "      <td>1525376</td>\n",
       "      <td>25158</td>\n",
       "      <td>114</td>\n",
       "      <td>356</td>\n",
       "      <td>844</td>\n",
       "      <td>1183</td>\n",
       "      <td>1181</td>\n",
       "      <td>94</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AcroBroker_exe.asm</td>\n",
       "      <td>12713416</td>\n",
       "      <td>204705</td>\n",
       "      <td>394</td>\n",
       "      <td>3431</td>\n",
       "      <td>5212</td>\n",
       "      <td>10113</td>\n",
       "      <td>10107</td>\n",
       "      <td>1213</td>\n",
       "      <td>3681</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 965 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    asm_id  asm_md_filesize  asm_md_loc  asm_symb_Star  \\\n",
       "0  64BitMAPIBroker_exe.asm          9316026      127559            408   \n",
       "1               7z_exe.asm         14349207      195374            733   \n",
       "2          ADelRCP_exe.asm          3930867       59845            193   \n",
       "3   ARPPRODUCTICON_exe.asm          1525376       25158            114   \n",
       "4       AcroBroker_exe.asm         12713416      204705            394   \n",
       "\n",
       "   asm_symb_Dash  asm_symb_Plus  asm_symb_Bracket_Open  \\\n",
       "0           4727          19269                  14054   \n",
       "1           6564          41002                  24276   \n",
       "2           5793           2723                   7484   \n",
       "3            356            844                   1183   \n",
       "4           3431           5212                  10113   \n",
       "\n",
       "   asm_symb_Bracket_Close  asm_symb_AtSign  asm_symb_Question  ...  \\\n",
       "0                   14066             1514              19061  ...   \n",
       "1                   24272              258              13061  ...   \n",
       "2                    7485              626               8642  ...   \n",
       "3                    1181               94                 46  ...   \n",
       "4                   10107             1213               3681  ...   \n",
       "\n",
       "   GetEnhMetaFilePaletteEntries  IsBadCodePtr  OpenMutexA  __vbaStrI4  \\\n",
       "0                             0             0           0           0   \n",
       "1                             0             0           0           0   \n",
       "2                             0             0           0           0   \n",
       "3                             0             0           0           0   \n",
       "4                             0             0           0           0   \n",
       "\n",
       "   __vbaVarCopy  CopyEnhMetaFileA  LocalReAlloc  srand  __vbaAryCopy  \\\n",
       "0             0                 0             0      0             0   \n",
       "1             0                 0             0      0             0   \n",
       "2             0                 0             0      0             0   \n",
       "3             0                 0             0      0             0   \n",
       "4             0                 0             0      0             0   \n",
       "\n",
       "   CreateDCA  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "\n",
       "[5 rows x 965 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_normal_x.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cc8f133-b65b-4fd2-8993-eebf012fabdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989, 1)\n"
     ]
    }
   ],
   "source": [
    "# create the labels for the normal data\n",
    "target_normal_y = np.zeros((target_normal_x.shape[0],1))\n",
    "print(target_normal_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e00192a2-386c-483b-9005-eb244791f5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989, 964)\n"
     ]
    }
   ],
   "source": [
    "# Drop the asm_id column\n",
    "target_normal_x = target_normal_x.drop(\"asm_id\", axis = 1)\n",
    "print(target_normal_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63409e16-d8da-4be3-966c-ecae4c551770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data using StandardScaler\n",
    "target_normal_x= StandardScaler().fit_transform(target_normal_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40ecd3-c006-494d-92fd-521da9b3483d",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a639b653-8338-4561-9161-039f719b4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN_NN(object):\n",
    "    def __init__(self, x_source_train, y_source_train, \n",
    "                 x_target_train, y_target_train, \n",
    "                 x_source_test, y_source_test, \n",
    "                 x_target_test, y_target_test,  nSteps=20000):\n",
    "\n",
    "        # Source train and test dataset\n",
    "        self.x_source_train = x_source_train\n",
    "        self.y_source_train = y_source_train\n",
    "        \n",
    "        self.x_source_test = x_source_test\n",
    "        self.y_source_test = y_source_test\n",
    "    \n",
    "        # Target train and test dataset\n",
    "        self.x_target_train = x_target_train\n",
    "        self.y_target_train = y_target_train\n",
    "        \n",
    "\n",
    "        self.x_target_test = x_target_test\n",
    "        self.y_target_test = y_target_test\n",
    "        \n",
    "     \n",
    "        \n",
    "        self.n_classes = y_source_train.shape[1]\n",
    "        \n",
    "\n",
    "                \n",
    "        # Use the source dataset shape for the generator input and outputs.\n",
    "        self.input_shape = x_source_train.shape[1]\n",
    "        self.output_shape = x_source_train.shape[1]\n",
    "        \n",
    "        #Latent dim for AE/VAE\n",
    "        self.latent_dim = 100\n",
    "        \n",
    "        self.optimizer_G = Adam(0.0002,0.5)\n",
    "        self.optimizer_D = Adam(0.0002,0.5) \n",
    "        self.optimizer_C = Adam(0.0002,0.5) \n",
    "         \n",
    "        self.batch_size = 128\n",
    "        self.nStep = nSteps\n",
    "\n",
    "\n",
    "    \n",
    "    def build_generator(self):\n",
    "        print(\"\\n== Build Generator...\")\n",
    "        \n",
    "        inputs = Input(self.input_shape)\n",
    "        net = Dense(units=100, activation=tf.nn.relu, name=\"fc_G1\")(inputs)\n",
    "        net = Dense(units=100, activation=tf.nn.relu, name=\"fc_G2\")(net)        \n",
    "        net = Dense(units=100, activation=tf.nn.relu, name=\"fc_G3\")(net)\n",
    "        net = Dense(units=100, activation=tf.nn.relu, name=\"fc_G4\")(net)\n",
    "\n",
    "        DIrep = Dense(units=self.latent_dim, activation=tf.nn.relu, name=\"DIrep\")(net)\n",
    "        G = Model(inputs=inputs, outputs=DIrep, name=\"Generator\")\n",
    "        \n",
    "        inputs = Input(shape=(self.latent_dim))\n",
    "        net = Dense(units=400, activation=tf.nn.relu, name=\"fc_C1\")(inputs)\n",
    "        net = Dense(units=400, activation=tf.nn.relu, name=\"fc_C2\")(net)\n",
    "        net = Dense(units=self.n_classes, activation=tf.nn.softmax, name=\"C\")(net)\n",
    "        C = Model(inputs=inputs, outputs=net, name=\"Classifier\")\n",
    "        \n",
    "        return G, C\n",
    "\n",
    "    def build_disciminator(self):\n",
    "        print(\"\\n== Build Discriminator...\")\n",
    "        \n",
    "        inputs = Input(self.latent_dim)\n",
    "        net = Dense(units=100, activation=tf.nn.relu, name=\"fc_D1\")(inputs)\n",
    "        net = Dense(units=100, activation=tf.nn.relu, name=\"fc_D2\")(net)\n",
    "        net = Dense(units=100, activation=tf.nn.relu, name=\"fc_D3\")(net)\n",
    "        net = Dense(units=100, activation=tf.nn.relu, name=\"fc_D4\")(net)\n",
    "\n",
    "        net = Dense(units=2, activation=tf.nn.softmax, name=\"D\")(net)\n",
    "        D = Model(inputs=inputs, outputs=net, name=\"Discriminator\")\n",
    "        return D\n",
    "\n",
    "\n",
    "    def d_loss(self, yhat_source, yhat_target): \n",
    "\n",
    "        y_source = np.tile([1,0], (yhat_source.shape[0], 1))\n",
    "        y_target = np.tile([0,1], (yhat_target.shape[0], 1))\n",
    "                          \n",
    "        bce = CategoricalCrossentropy(from_logits=False)        \n",
    "        return bce(y_source, yhat_source) + bce(y_target, yhat_target)\n",
    "\n",
    "    def g_loss(self, yhat_source, yhat_target):\n",
    "\n",
    "        y_source = np.tile([0,1], (yhat_source.shape[0], 1))\n",
    "        y_target = np.tile([1,0], (yhat_target.shape[0], 1))\n",
    "                          \n",
    "        bce = CategoricalCrossentropy(from_logits=False)        \n",
    "        return bce(y_source, yhat_source) + bce(y_target, yhat_target)\n",
    "\n",
    "    def c_loss(self, yhat_class_source, yhat_class_target, y_source, y_target):\n",
    "        \n",
    "        bce = CategoricalCrossentropy(from_logits=False)\n",
    "            \n",
    "        \n",
    "        return bce(y_source, yhat_class_source) + bce(y_target, yhat_class_target)*0.1\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        D = self.build_disciminator()\n",
    "        G, C = self.build_generator()\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # Create batch generators for source, target unlabeled and target labeled samples\n",
    "        S_batches = tf.data.Dataset.from_tensor_slices((self.x_source_train, self.y_source_train)).repeat().batch(self.batch_size).as_numpy_iterator()\n",
    "        T_batches = tf.data.Dataset.from_tensor_slices((self.x_target_train, self.y_target_train)).repeat().batch(self.batch_size).as_numpy_iterator()\n",
    "      \n",
    "        \n",
    "        #optimizer = self.optimizer\n",
    "\n",
    "        #g_loss_weight = 1\n",
    "        c_loss_weight = 1\n",
    "        g_loss_weight = 0.1\n",
    "        \n",
    "\n",
    "        print('====Loss Weights====')\n",
    "        print('g_loss_weight: {0}'.format(g_loss_weight))\n",
    "        print('c_loss_weight: {0}'.format(c_loss_weight))\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        def _train_step():\n",
    "            \n",
    "           \n",
    "\n",
    "            # Get a batch of source and target unlabeled samples\n",
    "            x_batch_source, y_batch_source = next(S_batches)\n",
    "            x_batch_target, y_batch_target = next(T_batches)\n",
    "            \n",
    "                 \n",
    "            #Create domain invariant mapping using the Generator\n",
    "            DIrep_source_samples = G(x_batch_source)\n",
    "            DIrep_target_samples = G(x_batch_target)\n",
    "\n",
    "           \n",
    "            # Calculate the Domain loss\n",
    "            with tf.GradientTape(persistent=True) as tape_disc:\n",
    "                \n",
    "                #Predict the domain using the discriminator\n",
    "                yhat_source = D(DIrep_source_samples)\n",
    "                yhat_target = D(DIrep_target_samples)\n",
    "                \n",
    "\n",
    "                # Compute D loss\n",
    "                d_loss_value = self.d_loss(yhat_source, yhat_target)\n",
    "                \n",
    "            # Given loss, compute and apply gradient for discriminator:\n",
    "            d_gradients = tape_disc.gradient(d_loss_value, D.trainable_variables)\n",
    "            self.optimizer_D.apply_gradients(zip(d_gradients, D.trainable_variables))\n",
    "            \n",
    "            ########################################################################\n",
    "            ########################################################################\n",
    "            ########################################################################\n",
    "            \n",
    "            # Get a batch of source and target unlabeled samples\n",
    "            x_batch_source, y_batch_source = next(S_batches)\n",
    "            x_batch_target, y_batch_target = next(T_batches)\n",
    "            \n",
    "\n",
    "\n",
    "            with tf.GradientTape(persistent=True) as tape_gen:\n",
    "                \n",
    "    \n",
    "                #Create domain invariant mapping using the Generator\n",
    "                DIrep_source_samples = G(x_batch_source)\n",
    "                DIrep_target_samples = G(x_batch_target)\n",
    "                \n",
    "                \n",
    "                #Predict the domain using the discriminator\n",
    "                yhat_source = D(DIrep_source_samples)\n",
    "                yhat_target = D(DIrep_target_samples)\n",
    "                \n",
    "\n",
    "                #Predict the class of the samples\n",
    "                class_pred_source = C(G(x_batch_source))\n",
    "                \n",
    "                \n",
    "                class_pred_target = C(G(x_batch_target))\n",
    "                \n",
    "            \n",
    "                # Compute G loss\n",
    "                g_loss_value = self.g_loss(yhat_source, yhat_target)\n",
    "                \n",
    "                # Compute C loss\n",
    "                c_loss_value = self.c_loss(class_pred_source, class_pred_target, \n",
    "                                          y_batch_source, y_batch_target)\n",
    "                \n",
    "                #combined_loss_value = (g_loss_weight * g_loss_value + c_loss_weight * c_loss_value) / (g_loss_weight+ c_loss_weight)\n",
    "\n",
    "                combined_loss_value = (g_loss_weight * g_loss_value \n",
    "                                       + c_loss_weight * c_loss_value) / (g_loss_weight + c_loss_weight)\n",
    "                \n",
    "            # Given loss, compute and apply gradient:            \n",
    "            g_gradients = tape_gen.gradient(combined_loss_value, G.trainable_variables)\n",
    "            c_gradients = tape_gen.gradient(c_loss_value, C.trainable_variables)\n",
    "            \n",
    "    \n",
    "            self.optimizer_G.apply_gradients(zip(g_gradients, G.trainable_variables))\n",
    "            self.optimizer_C.apply_gradients(zip(c_gradients, C.trainable_variables))\n",
    "            \n",
    "            return G, C, D, g_loss_value, c_loss_value, d_loss_value\n",
    "\n",
    "        # Start training nStep:\n",
    "        for step in range(self.nStep):\n",
    "            generator, classifier, discriminator,  \\\n",
    "            g_loss_value, c_loss_value, d_loss_value = _train_step()\n",
    "            \n",
    "            if step % 1000 == 0:\n",
    "                y_source_pred_test = classifier.predict(generator(self.x_source_test)).argmax(1)\n",
    "                y_target_pred_test = classifier.predict(generator(self.x_target_test)).argmax(1)\n",
    "                \n",
    "                accuracy_source = accuracy_score(self.y_source_test.argmax(1), y_source_pred_test)\n",
    "                accuracy_target = accuracy_score(self.y_target_test.argmax(1), y_target_pred_test)\n",
    "                f1_target = f1_score(self.y_target_test.argmax(1), y_target_pred_test)\n",
    "\n",
    "               \n",
    "                track_loss = ('Step {} ==> G_Loss: {} C_Loss: {} D_Loss: {}  \\n'\n",
    "                              'Acc Source: {} Acc Target: {} f1 Target: {} \\n' ).format(step, g_loss_value.numpy(), c_loss_value.numpy(), \n",
    "                                                                        d_loss_value.numpy(), accuracy_source, accuracy_target, f1_target)\n",
    "                                                                                    \n",
    "                print(track_loss)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        print('Training ended')\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679337b8-6cb6-48a3-b50a-1db382b39968",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8bf14-040a-4442-8b00-062e1e452367",
   "metadata": {},
   "source": [
    "## Set Cluster 0 as the target domain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a330654c-f90d-4073-8333-5c0f273a0e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malware data ...\n",
      "Target: (6023, 964)\n",
      "Target; (6023, 1)\n",
      "Source (4685, 964)\n",
      "Source (4685, 1)\n",
      "Normal data ...\n",
      "Target: (989, 964)\n",
      "Target: (989, 1)\n",
      "Source (6396, 964)\n",
      "Source (6396, 1)\n",
      "Combined data ...\n",
      "Target: (7012, 964)\n",
      "Target: (7012, 2)\n",
      "Source (11081, 964)\n",
      "Source (11081, 2)\n",
      "train test data ...\n",
      "Target train: (3506, 964)\n",
      "Target train: (3506, 2)\n",
      "Target test: (3506, 964)\n",
      "Target test: (3506, 2)\n",
      "Source train: (8310, 964)\n",
      "Source train: (8310, 2)\n",
      "Source test: (2771, 964)\n",
      "Source test: (2771, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_malware_x, target_malware_y, source_malware_x, source_malware_y =  filter_label(malware_x, malware_y, [0])\n",
    "print(\"Malware data ...\")\n",
    "print(\"Target: {}\".format(target_malware_x.shape))\n",
    "print(\"Target; {}\".format(target_malware_y.shape))\n",
    "print(\"Source {}\".format(source_malware_x.shape))\n",
    "print(\"Source {}\".format(source_malware_y.shape))\n",
    "\n",
    "print(\"Normal data ...\")\n",
    "print(\"Target: {}\".format(target_normal_x.shape))\n",
    "print(\"Target: {}\".format(target_normal_y.shape))\n",
    "print(\"Source {}\".format(source_normal_x.shape))\n",
    "print(\"Source {}\".format(source_normal_y.shape))\n",
    "\n",
    "#concatenate source and target\n",
    "source_malware_y = np.apply_along_axis(change_attack_label, 1, source_malware_y)\n",
    "target_malware_y = np.apply_along_axis(change_attack_label, 1, target_malware_y)\n",
    "\n",
    "\n",
    "source_x = np.concatenate((source_malware_x, source_normal_x), axis = 0)\n",
    "source_y = np.concatenate((source_malware_y, source_normal_y), axis = 0)\n",
    "\n",
    "\n",
    "target_x = np.concatenate((target_malware_x, target_normal_x), axis = 0)\n",
    "target_y = np.concatenate((target_malware_y, target_normal_y), axis = 0)\n",
    "\n",
    "\n",
    "#one-hot encode labels\n",
    "\n",
    "source_y = tf.keras.utils.to_categorical(source_y, num_classes = 2)\n",
    "target_y = tf.keras.utils.to_categorical(target_y, num_classes = 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Combined data ...\")\n",
    "print(\"Target: {}\".format(target_x.shape))\n",
    "print(\"Target: {}\".format(target_y.shape))\n",
    "print(\"Source {}\".format(source_x.shape))\n",
    "print(\"Source {}\".format(source_y.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into train and test sets\n",
    "source_x_train, source_x_test, source_y_train, source_y_test = train_test_split(source_x, source_y, test_size=0.25, random_state=42)\n",
    "target_x_train, target_x_test, target_y_train, target_y_test = train_test_split(target_x, target_y, test_size=0.5, random_state=42)\n",
    "        \n",
    "    \n",
    "\n",
    "print(\"train test data ...\")\n",
    "print(\"Target train: {}\".format(target_x_train.shape))\n",
    "print(\"Target train: {}\".format(target_y_train.shape))\n",
    "print(\"Target test: {}\".format(target_x_test.shape))\n",
    "print(\"Target test: {}\".format(target_y_test.shape))\n",
    "print(\"Source train: {}\".format(source_x_train.shape))\n",
    "print(\"Source train: {}\".format(source_y_train.shape))\n",
    "print(\"Source test: {}\".format(source_x_test.shape))\n",
    "print(\"Source test: {}\".format(source_y_test.shape))\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c68c1e5-d21e-4736-951e-61ba0517680a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------20 labeled samples per class ---------------------------\n",
      "target_x_train_select: (20, 964)\n",
      "target_y_train_select: (20, 2)\n",
      "source_x_train: (8310, 964)\n",
      "source_y_train: (8310, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "87/87 [==============================] - 0s 797us/step\n",
      "110/110 [==============================] - 0s 827us/step\n",
      "Step 0 ==> G_Loss: 1.3888676166534424 C_Loss: 0.7542629241943359 D_Loss: 1.388132095336914  \n",
      "Acc Source: 0.42547816672681343 Acc Target: 0.8593839132915003 f1 Target: 0.9243749041263998 \n",
      "\n",
      "87/87 [==============================] - 0s 809us/step\n",
      "110/110 [==============================] - 0s 760us/step\n",
      "Step 1000 ==> G_Loss: 1.6260439157485962 C_Loss: 0.001560297328978777 D_Loss: 1.3349717855453491  \n",
      "Acc Source: 0.9967520750631541 Acc Target: 0.905875641756988 f1 Target: 0.9474856779121579 \n",
      "\n",
      "Training ended\n",
      "---------------------50 labeled samples per class ---------------------------\n",
      "target_x_train_select: (50, 964)\n",
      "target_y_train_select: (50, 2)\n",
      "source_x_train: (8310, 964)\n",
      "source_y_train: (8310, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "87/87 [==============================] - 0s 801us/step\n",
      "110/110 [==============================] - 0s 745us/step\n",
      "Step 0 ==> G_Loss: 1.4164022207260132 C_Loss: 0.7709367275238037 D_Loss: 1.3702919483184814  \n",
      "Acc Source: 0.6254059906171058 Acc Target: 0.45436394751853965 f1 Target: 0.5615402246160898 \n",
      "\n",
      "87/87 [==============================] - 0s 813us/step\n",
      "110/110 [==============================] - 0s 781us/step\n",
      "Step 1000 ==> G_Loss: 1.8581340312957764 C_Loss: 0.0028216249775141478 D_Loss: 1.245095133781433  \n",
      "Acc Source: 0.9931432695777698 Acc Target: 0.9229891614375356 f1 Target: 0.9566195372750643 \n",
      "\n",
      "Training ended\n",
      "---------------------100 labeled samples per class ---------------------------\n",
      "target_x_train_select: (100, 964)\n",
      "target_y_train_select: (100, 2)\n",
      "source_x_train: (8310, 964)\n",
      "source_y_train: (8310, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "87/87 [==============================] - 0s 781us/step\n",
      "110/110 [==============================] - 0s 777us/step\n",
      "Step 0 ==> G_Loss: 1.3798773288726807 C_Loss: 0.757793128490448 D_Loss: 1.404613733291626  \n",
      "Acc Source: 0.45579213280404185 Acc Target: 0.8562464346833999 f1 Target: 0.9216904909881914 \n",
      "\n",
      "87/87 [==============================] - 0s 737us/step\n",
      "110/110 [==============================] - 0s 751us/step\n",
      "Step 1000 ==> G_Loss: 2.0690865516662598 C_Loss: 0.00016192768816836178 D_Loss: 1.1993764638900757  \n",
      "Acc Source: 0.9942259112233851 Acc Target: 0.9112949229891615 f1 Target: 0.9504540385534491 \n",
      "\n",
      "Training ended\n",
      "---------------------200 labeled samples per class ---------------------------\n",
      "target_x_train_select: (200, 964)\n",
      "target_y_train_select: (200, 2)\n",
      "source_x_train: (8310, 964)\n",
      "source_y_train: (8310, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "87/87 [==============================] - 0s 769us/step\n",
      "110/110 [==============================] - 0s 784us/step\n",
      "Step 0 ==> G_Loss: 1.3800468444824219 C_Loss: 0.7801905274391174 D_Loss: 1.394284725189209  \n",
      "Acc Source: 0.5925658607001083 Acc Target: 0.19652025099828865 f1 Target: 0.13189522342064716 \n",
      "\n",
      "87/87 [==============================] - 0s 849us/step\n",
      "110/110 [==============================] - 0s 756us/step\n",
      "Step 1000 ==> G_Loss: 1.9247817993164062 C_Loss: 0.0013211898040026426 D_Loss: 1.2524361610412598  \n",
      "Acc Source: 0.9935041501263082 Acc Target: 0.9366799771819737 f1 Target: 0.9640311082307194 \n",
      "\n",
      "Training ended\n",
      "---------------------300 labeled samples per class ---------------------------\n",
      "target_x_train_select: (300, 964)\n",
      "target_y_train_select: (300, 2)\n",
      "source_x_train: (8310, 964)\n",
      "source_y_train: (8310, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "87/87 [==============================] - 0s 768us/step\n",
      "110/110 [==============================] - 0s 771us/step\n",
      "Step 0 ==> G_Loss: 1.3836989402770996 C_Loss: 0.7668944597244263 D_Loss: 1.390803337097168  \n",
      "Acc Source: 0.6282930350054132 Acc Target: 0.6243582430119795 f1 Target: 0.7560659381366919 \n",
      "\n",
      "87/87 [==============================] - 0s 827us/step\n",
      "110/110 [==============================] - 0s 777us/step\n",
      "Step 1000 ==> G_Loss: 2.160417079925537 C_Loss: 0.0030809028539806604 D_Loss: 1.2721149921417236  \n",
      "Acc Source: 0.9913388668350775 Acc Target: 0.9383913291500285 f1 Target: 0.9648780487804878 \n",
      "\n",
      "Training ended\n",
      "---------------------500 labeled samples per class ---------------------------\n",
      "target_x_train_select: (500, 964)\n",
      "target_y_train_select: (500, 2)\n",
      "source_x_train: (8310, 964)\n",
      "source_y_train: (8310, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "87/87 [==============================] - 0s 731us/step\n",
      "110/110 [==============================] - 0s 797us/step\n",
      "Step 0 ==> G_Loss: 1.3843029737472534 C_Loss: 0.7496381402015686 D_Loss: 1.3975880146026611  \n",
      "Acc Source: 0.42547816672681343 Acc Target: 0.8593839132915003 f1 Target: 0.9243749041263998 \n",
      "\n",
      "87/87 [==============================] - 0s 808us/step\n",
      "110/110 [==============================] - 0s 811us/step\n",
      "Step 1000 ==> G_Loss: 2.2204413414001465 C_Loss: 0.0012788237072527409 D_Loss: 1.2540254592895508  \n",
      "Acc Source: 0.9960303139660772 Acc Target: 0.9489446662863662 f1 Target: 0.9708421567030462 \n",
      "\n",
      "Training ended\n"
     ]
    }
   ],
   "source": [
    "samples = [20,50,100,200,300,500]\n",
    "\n",
    "for size in samples: \n",
    "    print(\"---------------------{} labeled samples per class ---------------------------\".format(size))\n",
    "    # Select a random sample of the target data of different sizes\n",
    "    idxs = np.random.permutation(target_x_train.shape[0]) \n",
    "    split = int(size)\n",
    "    idx_sample, _= np.split(idxs, [split])\n",
    "    target_x_train_select =  target_x_train[idx_sample]\n",
    "    target_y_train_select =  target_y_train[idx_sample]\n",
    "    \n",
    "    print(\"target_x_train_select: {}\".format(target_x_train_select.shape))\n",
    "    print(\"target_y_train_select: {}\".format(target_y_train_select.shape))\n",
    "    print('source_x_train: {0}'.format(source_x_train.shape))\n",
    "    print('source_y_train: {0}'.format(source_y_train.shape))\n",
    "\n",
    "    # We have limited the number of training steps to 2000 to minimize training time.\n",
    "    # You can change the epochs to a lower number (lowest 1) just to test if the code is functional.\n",
    "    dann_nn = DANN_NN(source_x_train, source_y_train, \n",
    "                  target_x_train_select,  target_y_train_select , \n",
    "                  source_x_test, source_y_test, \n",
    "                  target_x_test,target_y_test, nSteps=2000)\n",
    "    \n",
    "    \n",
    "\n",
    "    dann_nn.train()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22143d32-3a95-41ee-87c8-9f6ca488316e",
   "metadata": {},
   "source": [
    "## Set Cluster 1 as the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6442bcc7-d486-4fd7-8f74-aab36ec18320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malware data ...\n",
      "Target: (3203, 964)\n",
      "Target; (3203, 1)\n",
      "Source (7505, 964)\n",
      "Source (7505, 1)\n",
      "Normal data ...\n",
      "Target: (989, 964)\n",
      "Target: (989, 1)\n",
      "Source (6396, 964)\n",
      "Source (6396, 1)\n",
      "Combined data ...\n",
      "Target: (4192, 964)\n",
      "Target: (4192, 2)\n",
      "Source (13901, 964)\n",
      "Source (13901, 2)\n",
      "train test data ...\n",
      "Target train: (2096, 964)\n",
      "Target train: (2096, 2)\n",
      "Target test: (2096, 964)\n",
      "Target test: (2096, 2)\n",
      "Source train: (10425, 964)\n",
      "Source train: (10425, 2)\n",
      "Source test: (3476, 964)\n",
      "Source test: (3476, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_malware_x, target_malware_y, source_malware_x, source_malware_y =  filter_label(malware_x, malware_y, [1])\n",
    "print(\"Malware data ...\")\n",
    "print(\"Target: {}\".format(target_malware_x.shape))\n",
    "print(\"Target; {}\".format(target_malware_y.shape))\n",
    "print(\"Source {}\".format(source_malware_x.shape))\n",
    "print(\"Source {}\".format(source_malware_y.shape))\n",
    "\n",
    "print(\"Normal data ...\")\n",
    "print(\"Target: {}\".format(target_normal_x.shape))\n",
    "print(\"Target: {}\".format(target_normal_y.shape))\n",
    "print(\"Source {}\".format(source_normal_x.shape))\n",
    "print(\"Source {}\".format(source_normal_y.shape))\n",
    "\n",
    "#concatenate source and target\n",
    "source_malware_y = np.apply_along_axis(change_attack_label, 1, source_malware_y)\n",
    "target_malware_y = np.apply_along_axis(change_attack_label, 1, target_malware_y)\n",
    "\n",
    "\n",
    "\n",
    "source_x = np.concatenate((source_malware_x, source_normal_x), axis = 0)\n",
    "source_y = np.concatenate((source_malware_y, source_normal_y), axis = 0)\n",
    "\n",
    "\n",
    "target_x = np.concatenate((target_malware_x, target_normal_x), axis = 0)\n",
    "target_y = np.concatenate((target_malware_y, target_normal_y), axis = 0)\n",
    "\n",
    "\n",
    "#one-hot encode labels\n",
    "\n",
    "source_y = tf.keras.utils.to_categorical(source_y, num_classes = 2)\n",
    "target_y = tf.keras.utils.to_categorical(target_y, num_classes = 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Combined data ...\")\n",
    "print(\"Target: {}\".format(target_x.shape))\n",
    "print(\"Target: {}\".format(target_y.shape))\n",
    "print(\"Source {}\".format(source_x.shape))\n",
    "print(\"Source {}\".format(source_y.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into train and test sets\n",
    "source_x_train, source_x_test, source_y_train, source_y_test = train_test_split(source_x, source_y, test_size=0.25, random_state=42)\n",
    "target_x_train, target_x_test, target_y_train, target_y_test = train_test_split(target_x, target_y, test_size=0.5, random_state=42)\n",
    "        \n",
    "    \n",
    "\n",
    "print(\"train test data ...\")\n",
    "print(\"Target train: {}\".format(target_x_train.shape))\n",
    "print(\"Target train: {}\".format(target_y_train.shape))\n",
    "print(\"Target test: {}\".format(target_x_test.shape))\n",
    "print(\"Target test: {}\".format(target_y_test.shape))\n",
    "print(\"Source train: {}\".format(source_x_train.shape))\n",
    "print(\"Source train: {}\".format(source_y_train.shape))\n",
    "print(\"Source test: {}\".format(source_x_test.shape))\n",
    "print(\"Source test: {}\".format(source_y_test.shape))\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f35c908b-99da-4b28-b4bf-70b5e652f4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------20 labeled samples per class ---------------------------\n",
      "target_x_train_select: (20, 964)\n",
      "target_y_train_select: (20, 2)\n",
      "source_x_train: (10425, 964)\n",
      "source_y_train: (10425, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "109/109 [==============================] - 0s 820us/step\n",
      "66/66 [==============================] - 0s 833us/step\n",
      "Step 0 ==> G_Loss: 1.3550982475280762 C_Loss: 0.7802437543869019 D_Loss: 1.4328595399856567  \n",
      "Acc Source: 0.4789988492520138 Acc Target: 0.3377862595419847 f1 Target: 0.2694736842105263 \n",
      "\n",
      "109/109 [==============================] - 0s 798us/step\n",
      "66/66 [==============================] - 0s 770us/step\n",
      "Step 1000 ==> G_Loss: 1.5842281579971313 C_Loss: 0.0016399379819631577 D_Loss: 1.3071002960205078  \n",
      "Acc Source: 0.9930955120828538 Acc Target: 0.8916984732824428 f1 Target: 0.9328997930830624 \n",
      "\n",
      "Training ended\n",
      "---------------------50 labeled samples per class ---------------------------\n",
      "target_x_train_select: (50, 964)\n",
      "target_y_train_select: (50, 2)\n",
      "source_x_train: (10425, 964)\n",
      "source_y_train: (10425, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "109/109 [==============================] - 0s 778us/step\n",
      "66/66 [==============================] - 0s 815us/step\n",
      "Step 0 ==> G_Loss: 1.393104910850525 C_Loss: 0.7691490054130554 D_Loss: 1.388810396194458  \n",
      "Acc Source: 0.5054660529344074 Acc Target: 0.43320610687022904 f1 Target: 0.49446808510638307 \n",
      "\n",
      "109/109 [==============================] - 0s 800us/step\n",
      "66/66 [==============================] - 0s 800us/step\n",
      "Step 1000 ==> G_Loss: 1.4504103660583496 C_Loss: 9.130154649028555e-05 D_Loss: 1.3548110723495483  \n",
      "Acc Source: 0.9933831990794016 Acc Target: 0.8874045801526718 f1 Target: 0.9310344827586207 \n",
      "\n",
      "Training ended\n",
      "---------------------100 labeled samples per class ---------------------------\n",
      "target_x_train_select: (100, 964)\n",
      "target_y_train_select: (100, 2)\n",
      "source_x_train: (10425, 964)\n",
      "source_y_train: (10425, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "109/109 [==============================] - 0s 758us/step\n",
      "66/66 [==============================] - 0s 806us/step\n",
      "Step 0 ==> G_Loss: 1.4205363988876343 C_Loss: 0.7651053667068481 D_Loss: 1.372737169265747  \n",
      "Acc Source: 0.5448791714614499 Acc Target: 0.6722328244274809 f1 Target: 0.8016170950043314 \n",
      "\n",
      "109/109 [==============================] - 0s 815us/step\n",
      "66/66 [==============================] - 0s 752us/step\n",
      "Step 1000 ==> G_Loss: 1.4965846538543701 C_Loss: 0.0001182135019917041 D_Loss: 1.3229267597198486  \n",
      "Acc Source: 0.9948216340621404 Acc Target: 0.896469465648855 f1 Target: 0.9361576934392469 \n",
      "\n",
      "Training ended\n",
      "---------------------200 labeled samples per class ---------------------------\n",
      "target_x_train_select: (200, 964)\n",
      "target_y_train_select: (200, 2)\n",
      "source_x_train: (10425, 964)\n",
      "source_y_train: (10425, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "109/109 [==============================] - 0s 787us/step\n",
      "66/66 [==============================] - 0s 787us/step\n",
      "Step 0 ==> G_Loss: 1.4156851768493652 C_Loss: 0.7617025971412659 D_Loss: 1.3766164779663086  \n",
      "Acc Source: 0.534234752589183 Acc Target: 0.75 f1 Target: 0.8561230093355299 \n",
      "\n",
      "109/109 [==============================] - 0s 804us/step\n",
      "66/66 [==============================] - 0s 755us/step\n",
      "Step 1000 ==> G_Loss: 1.550917387008667 C_Loss: 0.00016734912060201168 D_Loss: 1.2902734279632568  \n",
      "Acc Source: 0.9930955120828538 Acc Target: 0.9208015267175572 f1 Target: 0.9502398081534772 \n",
      "\n",
      "Training ended\n",
      "---------------------300 labeled samples per class ---------------------------\n",
      "target_x_train_select: (300, 964)\n",
      "target_y_train_select: (300, 2)\n",
      "source_x_train: (10425, 964)\n",
      "source_y_train: (10425, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "109/109 [==============================] - 0s 777us/step\n",
      "66/66 [==============================] - 0s 792us/step\n",
      "Step 0 ==> G_Loss: 1.390101671218872 C_Loss: 0.7601306438446045 D_Loss: 1.3854923248291016  \n",
      "Acc Source: 0.5302071346375143 Acc Target: 0.7652671755725191 f1 Target: 0.8662316476345839 \n",
      "\n",
      "109/109 [==============================] - 0s 802us/step\n",
      "66/66 [==============================] - 0s 778us/step\n",
      "Step 1000 ==> G_Loss: 1.493370771408081 C_Loss: 0.00016006550868041813 D_Loss: 1.3337628841400146  \n",
      "Acc Source: 0.992807825086306 Acc Target: 0.9379770992366412 f1 Target: 0.9607961399276237 \n",
      "\n",
      "Training ended\n",
      "---------------------500 labeled samples per class ---------------------------\n",
      "target_x_train_select: (500, 964)\n",
      "target_y_train_select: (500, 2)\n",
      "source_x_train: (10425, 964)\n",
      "source_y_train: (10425, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "109/109 [==============================] - 0s 787us/step\n",
      "66/66 [==============================] - 0s 794us/step\n",
      "Step 0 ==> G_Loss: 1.4120805263519287 C_Loss: 0.7538780570030212 D_Loss: 1.3681992292404175  \n",
      "Acc Source: 0.5307825086306099 Acc Target: 0.7643129770992366 f1 Target: 0.8664142779881016 \n",
      "\n",
      "109/109 [==============================] - 0s 824us/step\n",
      "66/66 [==============================] - 0s 764us/step\n",
      "Step 1000 ==> G_Loss: 1.4436341524124146 C_Loss: 0.0002455127832945436 D_Loss: 1.347623348236084  \n",
      "Acc Source: 0.9948216340621404 Acc Target: 0.941793893129771 f1 Target: 0.9630750605326877 \n",
      "\n",
      "Training ended\n"
     ]
    }
   ],
   "source": [
    "samples = [20,50,100,200,300,500]\n",
    "\n",
    "for size in samples: \n",
    "    print(\"---------------------{} labeled samples per class ---------------------------\".format(size))\n",
    "    # Select a random sample of the target data of different sizes\n",
    "    idxs = np.random.permutation(target_x_train.shape[0]) \n",
    "    split = int(size)\n",
    "    idx_sample, _= np.split(idxs, [split])\n",
    "    target_x_train_select =  target_x_train[idx_sample]\n",
    "    target_y_train_select =  target_y_train[idx_sample]\n",
    "    \n",
    "    print(\"target_x_train_select: {}\".format(target_x_train_select.shape))\n",
    "    print(\"target_y_train_select: {}\".format(target_y_train_select.shape))\n",
    "    print('source_x_train: {0}'.format(source_x_train.shape))\n",
    "    print('source_y_train: {0}'.format(source_y_train.shape))\n",
    "\n",
    "\n",
    "    # We have limited the number of training steps to 2000 to minimize training time.\n",
    "    # You can change the epochs to a lower number (lowest 1) just to test if the code is functional.\n",
    "    dann_nn = DANN_NN(source_x_train, source_y_train, \n",
    "                  target_x_train_select,  target_y_train_select , \n",
    "                  source_x_test, source_y_test, \n",
    "                  target_x_test,target_y_test, nSteps=2000)\n",
    "    \n",
    "\n",
    "    dann_nn.train()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724c9b6-ead1-4775-95be-661c7be01527",
   "metadata": {},
   "source": [
    "## Set Cluster 2 as the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28919335-8066-4ba9-b5d7-1a4a957625ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malware data ...\n",
      "Target: (1438, 964)\n",
      "Target; (1438, 1)\n",
      "Source (9270, 964)\n",
      "Source (9270, 1)\n",
      "Normal data ...\n",
      "Target: (989, 964)\n",
      "Target: (989, 1)\n",
      "Source (6396, 964)\n",
      "Source (6396, 1)\n",
      "Combined data ...\n",
      "Target: (2427, 964)\n",
      "Target: (2427, 2)\n",
      "Source (15666, 964)\n",
      "Source (15666, 2)\n",
      "train test data ...\n",
      "Target train: (1213, 964)\n",
      "Target train: (1213, 2)\n",
      "Target test: (1214, 964)\n",
      "Target test: (1214, 2)\n",
      "Source train: (11749, 964)\n",
      "Source train: (11749, 2)\n",
      "Source test: (3917, 964)\n",
      "Source test: (3917, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_malware_x, target_malware_y, source_malware_x, source_malware_y =  filter_label(malware_x, malware_y, [2])\n",
    "print(\"Malware data ...\")\n",
    "print(\"Target: {}\".format(target_malware_x.shape))\n",
    "print(\"Target; {}\".format(target_malware_y.shape))\n",
    "print(\"Source {}\".format(source_malware_x.shape))\n",
    "print(\"Source {}\".format(source_malware_y.shape))\n",
    "\n",
    "print(\"Normal data ...\")\n",
    "print(\"Target: {}\".format(target_normal_x.shape))\n",
    "print(\"Target: {}\".format(target_normal_y.shape))\n",
    "print(\"Source {}\".format(source_normal_x.shape))\n",
    "print(\"Source {}\".format(source_normal_y.shape))\n",
    "\n",
    "#concatenate source and target\n",
    "source_malware_y = np.apply_along_axis(change_attack_label, 1, source_malware_y)\n",
    "target_malware_y = np.apply_along_axis(change_attack_label, 1, target_malware_y)\n",
    "\n",
    "\n",
    "source_x = np.concatenate((source_malware_x, source_normal_x), axis = 0)\n",
    "source_y = np.concatenate((source_malware_y, source_normal_y), axis = 0)\n",
    "\n",
    "\n",
    "target_x = np.concatenate((target_malware_x, target_normal_x), axis = 0)\n",
    "target_y = np.concatenate((target_malware_y, target_normal_y), axis = 0)\n",
    "\n",
    "\n",
    "#one-hot encode labels\n",
    "\n",
    "source_y = tf.keras.utils.to_categorical(source_y, num_classes = 2)\n",
    "target_y = tf.keras.utils.to_categorical(target_y, num_classes = 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Combined data ...\")\n",
    "print(\"Target: {}\".format(target_x.shape))\n",
    "print(\"Target: {}\".format(target_y.shape))\n",
    "print(\"Source {}\".format(source_x.shape))\n",
    "print(\"Source {}\".format(source_y.shape))\n",
    "\n",
    "# Split the data into train and test sets\n",
    "source_x_train, source_x_test, source_y_train, source_y_test = train_test_split(source_x, source_y, test_size=0.25,random_state=42)\n",
    "target_x_train, target_x_test, target_y_train, target_y_test = train_test_split(target_x, target_y, test_size=0.5,random_state=42)\n",
    "        \n",
    "    \n",
    "\n",
    "print(\"train test data ...\")\n",
    "print(\"Target train: {}\".format(target_x_train.shape))\n",
    "print(\"Target train: {}\".format(target_y_train.shape))\n",
    "print(\"Target test: {}\".format(target_x_test.shape))\n",
    "print(\"Target test: {}\".format(target_y_test.shape))\n",
    "print(\"Source train: {}\".format(source_x_train.shape))\n",
    "print(\"Source train: {}\".format(source_y_train.shape))\n",
    "print(\"Source test: {}\".format(source_x_test.shape))\n",
    "print(\"Source test: {}\".format(source_y_test.shape))\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ffaeaf7-31c2-4f09-92ac-f19ca964ccf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------20 labeled samples per class ---------------------------\n",
      "target_x_train_select: (20, 964)\n",
      "target_y_train_select: (20, 2)\n",
      "source_x_train: (11749, 964)\n",
      "source_y_train: (11749, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "123/123 [==============================] - 0s 783us/step\n",
      "38/38 [==============================] - 0s 825us/step\n",
      "Step 0 ==> G_Loss: 1.3954771757125854 C_Loss: 0.7550563216209412 D_Loss: 1.3805010318756104  \n",
      "Acc Source: 0.5966300740362522 Acc Target: 0.6046128500823723 f1 Target: 0.7530864197530864 \n",
      "\n",
      "123/123 [==============================] - 0s 754us/step\n",
      "38/38 [==============================] - 0s 740us/step\n",
      "Step 1000 ==> G_Loss: 1.556349277496338 C_Loss: 0.0003473341348581016 D_Loss: 1.3148460388183594  \n",
      "Acc Source: 0.9943834567270871 Acc Target: 0.7462932454695222 f1 Target: 0.826381059751973 \n",
      "\n",
      "Training ended\n",
      "---------------------50 labeled samples per class ---------------------------\n",
      "target_x_train_select: (50, 964)\n",
      "target_y_train_select: (50, 2)\n",
      "source_x_train: (11749, 964)\n",
      "source_y_train: (11749, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "123/123 [==============================] - 0s 772us/step\n",
      "38/38 [==============================] - 0s 801us/step\n",
      "Step 0 ==> G_Loss: 1.3884150981903076 C_Loss: 0.7559513449668884 D_Loss: 1.3862695693969727  \n",
      "Acc Source: 0.5973959663007403 Acc Target: 0.6037891268533773 f1 Target: 0.7529532614278377 \n",
      "\n",
      "123/123 [==============================] - 0s 820us/step\n",
      "38/38 [==============================] - 0s 808us/step\n",
      "Step 1000 ==> G_Loss: 1.4336507320404053 C_Loss: 0.00010209363972535357 D_Loss: 1.3589951992034912  \n",
      "Acc Source: 0.9941281593055911 Acc Target: 0.8657331136738056 f1 Target: 0.8999386126457949 \n",
      "\n",
      "Training ended\n",
      "---------------------100 labeled samples per class ---------------------------\n",
      "target_x_train_select: (100, 964)\n",
      "target_y_train_select: (100, 2)\n",
      "source_x_train: (11749, 964)\n",
      "source_y_train: (11749, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "123/123 [==============================] - 0s 806us/step\n",
      "38/38 [==============================] - 0s 785us/step\n",
      "Step 0 ==> G_Loss: 1.3847662210464478 C_Loss: 0.7806777954101562 D_Loss: 1.3870733976364136  \n",
      "Acc Source: 0.43911156497319376 Acc Target: 0.42915980230642503 f1 Target: 0.37958818263205013 \n",
      "\n",
      "123/123 [==============================] - 0s 867us/step\n",
      "38/38 [==============================] - 0s 1ms/step\n",
      "Step 1000 ==> G_Loss: 1.4055486917495728 C_Loss: 0.00010919597116298974 D_Loss: 1.3551688194274902  \n",
      "Acc Source: 0.9951493489915751 Acc Target: 0.8657331136738056 f1 Target: 0.8999386126457949 \n",
      "\n",
      "Training ended\n",
      "---------------------200 labeled samples per class ---------------------------\n",
      "target_x_train_select: (200, 964)\n",
      "target_y_train_select: (200, 2)\n",
      "source_x_train: (11749, 964)\n",
      "source_y_train: (11749, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "123/123 [==============================] - 0s 2ms/step\n",
      "38/38 [==============================] - 0s 812us/step\n",
      "Step 0 ==> G_Loss: 1.3919668197631836 C_Loss: 0.790267825126648 D_Loss: 1.3825103044509888  \n",
      "Acc Source: 0.5082971661986214 Acc Target: 0.5889621087314663 f1 Target: 0.673215455140799 \n",
      "\n",
      "123/123 [==============================] - 0s 794us/step\n",
      "38/38 [==============================] - 0s 774us/step\n",
      "Step 1000 ==> G_Loss: 1.3955957889556885 C_Loss: 0.00015853409422561526 D_Loss: 1.3735899925231934  \n",
      "Acc Source: 0.993872861884095 Acc Target: 0.9110378912685337 f1 Target: 0.9312101910828026 \n",
      "\n",
      "Training ended\n",
      "---------------------300 labeled samples per class ---------------------------\n",
      "target_x_train_select: (300, 964)\n",
      "target_y_train_select: (300, 2)\n",
      "source_x_train: (11749, 964)\n",
      "source_y_train: (11749, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "123/123 [==============================] - 0s 764us/step\n",
      "38/38 [==============================] - 0s 751us/step\n",
      "Step 0 ==> G_Loss: 1.4039140939712524 C_Loss: 0.7537122368812561 D_Loss: 1.377554178237915  \n",
      "Acc Source: 0.5973959663007403 Acc Target: 0.6046128500823723 f1 Target: 0.7533401849948613 \n",
      "\n",
      "123/123 [==============================] - 0s 797us/step\n",
      "38/38 [==============================] - 0s 761us/step\n",
      "Step 1000 ==> G_Loss: 1.3914368152618408 C_Loss: 6.976284203119576e-05 D_Loss: 1.3677000999450684  \n",
      "Acc Source: 0.9959152412560633 Acc Target: 0.9192751235584844 f1 Target: 0.9373401534526854 \n",
      "\n",
      "Training ended\n",
      "---------------------500 labeled samples per class ---------------------------\n",
      "target_x_train_select: (500, 964)\n",
      "target_y_train_select: (500, 2)\n",
      "source_x_train: (11749, 964)\n",
      "source_y_train: (11749, 2)\n",
      "\n",
      "== Build Discriminator...\n",
      "\n",
      "== Build Generator...\n",
      "====Loss Weights====\n",
      "g_loss_weight: 0.1\n",
      "c_loss_weight: 1\n",
      "123/123 [==============================] - 0s 811us/step\n",
      "38/38 [==============================] - 0s 794us/step\n",
      "Step 0 ==> G_Loss: 1.386747121810913 C_Loss: 0.740990400314331 D_Loss: 1.3900806903839111  \n",
      "Acc Source: 0.5973959663007403 Acc Target: 0.6037891268533773 f1 Target: 0.7529532614278377 \n",
      "\n",
      "123/123 [==============================] - 0s 791us/step\n",
      "38/38 [==============================] - 0s 836us/step\n",
      "Step 1000 ==> G_Loss: 1.410494327545166 C_Loss: 7.812822877895087e-05 D_Loss: 1.3784432411193848  \n",
      "Acc Source: 0.9948940515700792 Acc Target: 0.9472817133443163 f1 Target: 0.9581151832460734 \n",
      "\n",
      "Training ended\n"
     ]
    }
   ],
   "source": [
    "samples = [20,50,100,200,300,500]\n",
    "\n",
    "for size in samples: \n",
    "    print(\"---------------------{} labeled samples per class ---------------------------\".format(size))\n",
    "    # Select a random sample of the target data of different sizes \n",
    "    idxs = np.random.permutation(target_x_train.shape[0]) \n",
    "    split = int(size)\n",
    "    idx_sample, _= np.split(idxs, [split])\n",
    "    target_x_train_select =  target_x_train[idx_sample]\n",
    "    target_y_train_select =  target_y_train[idx_sample]\n",
    "    \n",
    "    print(\"target_x_train_select: {}\".format(target_x_train_select.shape))\n",
    "    print(\"target_y_train_select: {}\".format(target_y_train_select.shape))\n",
    "    print('source_x_train: {0}'.format(source_x_train.shape))\n",
    "    print('source_y_train: {0}'.format(source_y_train.shape))\n",
    "\n",
    "\n",
    "    # We have limited the number of training steps to 2000 to minimize training time.\n",
    "    # You can change the epochs to a lower number (lowest 1) just to test if the code is functional.\n",
    "    dann_nn = DANN_NN(source_x_train, source_y_train, \n",
    "                  target_x_train_select,  target_y_train_select , \n",
    "                  source_x_test, source_y_test, \n",
    "                  target_x_test,target_y_test, nSteps=2000)\n",
    "    \n",
    "\n",
    "    dann_nn.train()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e3f1fa-e7ee-40a4-bb9c-aeed01fc9953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ipykernel",
   "language": "python",
   "name": "ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
