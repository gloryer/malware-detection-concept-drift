# malware-detection-concept-drift


## Hardware Specifications
We have successfully run the code with the following hardware
* CPU: Intel® Core™ i7-6700K CPU @ 4.00GHz × 8
* GPU: Nvidia RTX 3090 (24GB)
* Memory: 64GB.
* Additionally, we recommend 50 GB of available disk space to store the data. 


## OS & Software Specifications
* The code was tested on Ubuntu 20.04 LTS and using python3. It should also run on Ubuntu 22.04 LTS and later stable versions.
* Most of the code is built with TensorFlow, but [PalmTree](https://github.com/palmtreemodel/PalmTree) was developed using PyTorch, so some notebooks also require PyTorch installation. The versions used are TensorFlow 2.9.0 and PyTorch 2.4.

## Installation
Please follow these steps to set up the environment (*For all the steps, we assume the current directory is:* ```malware-detection-concept-drift```)
* Download the [data](https://app.box.com/s/bu6mv33g58h6gxiu0wgpcx4lpyzkdd0k) from Box and place it under the current directory. It has all the data required to run the following experiments.
* Run ```tar -xzvf data.tar.gz``` to unpack the compressed file and **do not change the name of the extracted folder** (the name should be ```/data```)
* Run ```pip install -r requirements.txt``` to install the required packages. (Note: The requirements were generated using `pip freeze` and modified manually to consider only the required packages.)

## Running the experiments
1.  **[CFG Extraction]** The following experiment is to extract cfgs from assembly files of malware/benign binaries. 
     * From the CFG directory , run the notebook (If you are using Jupyter GUI, you may do `Kernel`>`Restart & Run All` for each of them)-
        - *extract_cfg.ipynb* 
    We only inclue five example asm files from Big-15 dataset to show the code is functional. But the code is scalable to process a large number of files. You can aslo view the node(s) and edges printed in pretty json in the second cell. 
	
2. **[Vertex Feature Extraction]** The following experiment is to generate the embeddings for the nodes of cfg using the pre-trained PalmTree model.
     * From the CFG directory , run the notebook (If you are using Jupyter GUI, you may do `Kernel`>`Restart & Run All` for each of them)-
        - *generate_embeddings.ipynb* 
    We only include five cfgs to show the code is functional. But the code is scalable and can perform over a very large number of files. 
	
3. **[Graph-based Clustering Phase 1]** The following experiment is to generate the graph embedding with a graph autoencoder. The graph embeddings will be used in Phase 2 of the graph-based clustering. 
   * From the graph_based_clustering directory , run the notebook (If you are using Jupyter GUI, you may do `Kernel`>`Restart & Run All` for each of them)-
        - *generate_graph_embeddings.ipynb* 
    To simplify the amount of work for testing, we demonstrate with just five graphs obtained from step 2, but the code is designed to process many graphs.
	
4. **[Graph-based Clustering Phase 2]** The following experiment is to generate the cluster labels with our weighted concensus clustering algorithm. 
   * From the graph_based_clustering directory , run the notebook (If you are using Jupyter GUI, you may do `Kernel`>`Restart & Run All` for each of them)-
     - *concensus_clustering.ipynb* 
Within the notebook, you'll find cells for t-SNE visualizations of the Big-15 dataset, showing both the original labels and the new cluster labels. Each cell will generate a figure in the output, which can be compared to Figure 4 in the paper.

5. **[Domain Adaptation]** The following experiment is to generate the our models' performance metrics on the Big-15 datasets 
  
   Follow these instructions to train our model-
    * Run the following notebooks in any order  (If you are using Jupyter GUI, you may do `Kernel`>`Restart & Run All` for each of them)-
        - *Train_origin_label_graph.ipynb*,
        - *Train_cluster_label_graph.ipynb*,
        - *Train_cluster_label_image.ipynb*,
        - *Train_cluster_label_content.ipynb*
      
     The first two notebooks will generate the metrics for our approach, as shown in Figure 3 of the paper. The third and fourth notebooks will generate the metrics for our approach using image representations and content-based representations, respectively, corresponding to Table IV of the paper.


