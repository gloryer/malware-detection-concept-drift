
import os
import numpy as np
from clustering.utils import *
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
from sklearn.decomposition import PCA
import pandas as pd

import hdbscan
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture



store_path = '/home/data/malware-classification/graph_embeddings_GAE'

filelist = os.listdir(store_path)

graph_features = []
label = []
asm_id = []

for filepath in filelist:

    fullpath = os.path.join(store_path, filepath)

    # with open(fullpath, 'rb') as f2:
    data = np.load(fullpath)

    if data["success"]:
        if len(data["feature"]) != 16:
            print("We have mismatch embeddings, length is {}".format(len(data["feature"])))

        graph_features.append(np.reshape(data["feature"], [-1]))
        label.append(np.reshape(data["y"], [-1]))
        asm_id.append(np.reshape(data["asm_id"], [-1]))


graph_features = np.array(graph_features)
label = np.array(label)
asm_id = np.array(asm_id)

#scaling the graph features
scale_graph_features = StandardScaler().fit_transform(graph_features)


hdb = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=2).fit(scale_graph_features)
hdbscan_labels = hdb.labels_

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(hdbscan_labels)) - (1 if -1 in hdbscan_labels else 0)
n_noise_ = list(hdbscan_labels).count(-1)

print("Estimated number of clusters: %d" % n_clusters_)
print("Estimated number of noise points: %d" % n_noise_)


# Calculating the metrics
print("HDBSCAN clusters performance evaluation:")
print(metrics.silhouette_score(scale_graph_features , hdbscan_labels, metric='euclidean'))
print(metrics.calinski_harabasz_score(scale_graph_features ,hdbscan_labels))
print(metrics.davies_bouldin_score(scale_graph_features , hdbscan_labels))



kmeans = KMeans(n_clusters=4, random_state=0,  init="k-means++").fit(scale_graph_features)
kmeans_labels = kmeans.labels_

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(kmeans_labels)) - (1 if -1 in kmeans_labels else 0)
n_noise_ = list(kmeans_labels).count(-1)

print("Estimated number of clusters: %d" % n_clusters_)
print("Estimated number of noise points: %d" % n_noise_)


print("Kmeans clusters performance evaluation:")
print(metrics.silhouette_score(scale_graph_features , kmeans_labels, metric='euclidean'))
print(metrics.calinski_harabasz_score(scale_graph_features ,kmeans_labels))
print(metrics.davies_bouldin_score(scale_graph_features , kmeans_labels))



gm = GaussianMixture(n_components=4, covariance_type = "full", random_state=0).fit(scale_graph_features)

gmm_labels = gm.predict(scale_graph_features)

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(gmm_labels)) - (1 if -1 in gmm_labels else 0)
n_noise_ = list(gmm_labels).count(-1)

print("Estimated number of clusters: %d" % n_clusters_)
print("Estimated number of noise points: %d" % n_noise_)


print("GMM clusters performance evaluation:")
print(metrics.silhouette_score(scale_graph_features , gmm_labels, metric='euclidean'))
print(metrics.calinski_harabasz_score(scale_graph_features ,gmm_labels))
print(metrics.davies_bouldin_score(scale_graph_features , gmm_labels))

cluster_assignments ={#"original":label,
           "hdbscan": hdbscan_labels,
           "kmeans":kmeans_labels,
           "gmm":gmm_labels}

coefficient_assignments = {#"original":(-0.11+1)/2,
           "hdbscan": (0.23+1)/2,
           "kmeans":(0.96+1)/2,
           "gmm":(-0.033+1)/2}



N = scale_graph_features.shape[0]
consensus_matrix = np.zeros((N, N))

# Assume cluster_assignments is a list of K arrays representing cluster assignments for each solution
for k in cluster_assignments.keys():
    cluster_assignment_k = cluster_assignments[k]
    coefficient_assignment_k = coefficient_assignments[k]

    for i in range(N):
        for j in range(i + 1, N):
            # Check if data points i and j are clustered together in solution k
            if cluster_assignment_k[i] != -1 and cluster_assignment_k[j] != -1 and cluster_assignment_k[i] == \
                    cluster_assignment_k[j]:
                consensus_matrix[i, j] += 1 * coefficient_assignment_k
                consensus_matrix[j, i] += 1 * coefficient_assignment_k

consensus_matrix = consensus_matrix / 3.0

print(consensus_matrix.shape)


pca = PCA(30)
new_consensus_matrix = pca.fit_transform(consensus_matrix)

#print(sum(pca.explained_variance_ratio_))


gm = GaussianMixture(n_components=4, random_state=0, covariance_type = "diag").fit(new_consensus_matrix)

gmm_concensus_labels = gm.predict(new_consensus_matrix)

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(gmm_concensus_labels )) - (1 if -1 in gmm_concensus_labels  else 0)
n_noise_ = list(gmm_concensus_labels ).count(-1)

print("Estimated number of clusters: %d" % n_clusters_)
print("Estimated number of noise points: %d" % n_noise_)

print("Concensus clusters performance evaluation:")
print(metrics.silhouette_score(scale_graph_features , gmm_concensus_labels, metric='euclidean'))
print(metrics.calinski_harabasz_score(scale_graph_features ,gmm_concensus_labels))
print(metrics.davies_bouldin_score(scale_graph_features , gmm_concensus_labels))

#save to csv
gmm_concensus_labels = np.reshape(gmm_concensus_labels, [-1, 1])
data_label_gmm= np.concatenate((asm_id, gmm_concensus_labels), 1)
print(data_label_gmm.shape)

data_label_gmm= pd.DataFrame(data_label_gmm,columns=['asm_id', "Class"])

data_label_gmm.to_csv("/home/data/GAE_labels_weighted/new_gmm_labels_id.csv")
