{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ba5b24-7613-4ae7-b66c-941d3f5ff941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from spektral.data import Dataset, DisjointLoader, Graph\n",
    "from spektral.layers import GINConv, GlobalAvgPool\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aee62f-71c9-47df-90fa-8e40abb7be02",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Load CFG json files as spektral graph objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "759ed7f5-4d27-485e-9289-ac8e293b3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Load data\n",
    "################################################################################ \n",
    "class GraphData(Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, cfg_path, **kwargs):\n",
    "        self.cfg_path = cfg_path\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        \n",
    "        file_list = os.listdir(self.cfg_path)\n",
    "        file_list_x_y = list(filter(lambda x: '_sparse_matrix' not in x and '.npz' in x, file_list))\n",
    "        \n",
    "        #print(len(file_list_x_y))\n",
    "        output = []\n",
    "        \n",
    "        \n",
    "       \n",
    "        for filepath in file_list_x_y:\n",
    "\n",
    "            #full path of node attribute and label\n",
    "            fullpath = os.path.join(self.cfg_path, filepath)\n",
    "            #file path of adj matrix\n",
    "            filepath_sp = filepath.split('.')[0] + \"_sparse_matrix.npz\"\n",
    "            #full path pf adj matrix\n",
    "            fullpath_sp = os.path.join(self.cfg_path, filepath_sp)\n",
    "            #with open(fullpath_sp, 'rb') as f1:\n",
    "            sparse_matrix = sp.load_npz(fullpath_sp)\n",
    "            sparse_matrix = sparse_matrix.astype('float32')\n",
    "           \n",
    "            #with open(fullpath, 'rb') as f2:\n",
    "            data = np.load(fullpath)\n",
    "            \n",
    "            # Remove diagonal elements\n",
    "            adj = sparse_matrix - sp.dia_matrix((sparse_matrix.diagonal()[np.newaxis, :], [0]), shape=sparse_matrix.shape)\n",
    "            adj.eliminate_zeros()\n",
    "            # Check that diag is zero:\n",
    "            assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "            adj_triu = sp.triu(adj)\n",
    "            adj_tuple = sparse_to_tuple(adj_triu)\n",
    "            edges = adj_tuple[0]\n",
    "        \n",
    "                \n",
    "   \n",
    "            \n",
    "            #important! filter out noisy data where the number of basic block is less than 10 and the number of non-self edges in the upper triangle is less than 3\n",
    "            # if the graph is too large, we will run into oom problems\n",
    "            if data[\"x\"].shape[0] >=10 and edges.shape[0] >= 3 and sparse_matrix.shape[0] <=46000:\n",
    "                output.append(Graph(x=data['x'], a= sparse_matrix, y=data['y']))\n",
    "                \n",
    "               \n",
    "          \n",
    "          \n",
    "\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8995cfd-6078-44cd-a043-0fa031ac7001",
   "metadata": {},
   "source": [
    "# Load malware data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9988f7ac-48f1-4895-bfaf-ac64b1828be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  'data/graph_features/big-15/cfg_embeddings/cfg_embeddings'\n",
    "malware = GraphData(path) \n",
    "malware = convert_label_integer(malware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e46a5cd4-ad81-4c4c-a317-4b0e13503b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 14.032303239660163, 1: 23.116422369526656, 2: 27.252357389599478, 3: 3.995892073569228, 4: 0.3921202502100644, 5: 6.862104378676127, 6: 3.5850994304920176, 7: 11.31547007749043, 8: 9.448230790775838}\n"
     ]
    }
   ],
   "source": [
    "label = obtain_labels(malware)\n",
    "uniques, counts = np.unique(label, return_counts=True)\n",
    "\n",
    "percentages = dict(zip(uniques, counts * 100 / len(label)))\n",
    "\n",
    "print(percentages)\n",
    "\n",
    "\n",
    "# 1 Ramnit \t1541 \tWorm 1\n",
    "# 2 Lollipop \t2478 \tAdware \n",
    "# 3 Kelihos_ver3 \t2942 \tBackdoor\n",
    "# 4 Vundo \t475 \tTrojan\n",
    "# 5 Simda \t42 \tBackdoor\n",
    "# 6 Tracur \t751 \tTrojan Downloader\n",
    "# 7 Kelihos_ver1 \t398 \tBackdoor\n",
    "# 8 Obfuscator.ACY \t1228 \tObfuscated malware\n",
    "# 9 Gatak \t1013 \tBackdoor\n",
    "# 0 Normal \t6392 \t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680be95-a01b-4361-a157-cd9d1af8d3b4",
   "metadata": {},
   "source": [
    "# Load benign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44f9924f-4a2b-4531-839f-7323857f86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  'data/graph_features/binary_corp/cfg_embeddings'\n",
    "normal = GraphData(path)\n",
    "normal = convert_label_integer(normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9e30f-ff43-403b-91a4-d693892f2b47",
   "metadata": {},
   "source": [
    "# Load benign data windows-PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d61d2b2e-3b8e-4673-aa93-a1935b1330ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  'data/graph_features/windows_PE/cfg_embeddings'\n",
    "normal_PE = GraphData(path)\n",
    "normal_PE = convert_label_integer(normal_PE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42617cf-e746-47d5-b09d-eb59e968505e",
   "metadata": {},
   "source": [
    "# Build our DA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11cb44bb-eb55-42c9-a528-82508c0a171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "# Build model\n",
    "################################################################################\n",
    "class GIN0(Model):\n",
    "    def __init__(self, channels, n_layers):\n",
    "        super().__init__()\n",
    "        self.conv1 = GINConv(channels, epsilon=0, mlp_hidden=[channels, channels])\n",
    "        self.convs = []\n",
    "        for _ in range(1, n_layers):\n",
    "            self.convs.append(\n",
    "                GINConv(channels, epsilon=0, mlp_hidden=[channels, channels])\n",
    "            )\n",
    "        self.pool = GlobalAvgPool()\n",
    "        self.dense1 = Dense(channels, activation=\"relu\")\n",
    "      \n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, i = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        for conv in self.convs:\n",
    "            x = conv([x, a])\n",
    "        x = self.pool([x, i])\n",
    "        x = self.dense1(x)\n",
    "      \n",
    "        return x\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da3aa825-0318-4b57-bf5a-8568c7544203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN_GIN(object):\n",
    "    def __init__(self,loader_source_train, \n",
    "                 loader_target_train,\n",
    "                 loader_target_test, GIN, n_classes,\n",
    "                 epochs=90):\n",
    "\n",
    "        #source train and test dataset\n",
    "        self.loader_source_tr = loader_source_train\n",
    "        #self.loader_source_te= loader_source_test\n",
    "        \n",
    "        \n",
    "        # Target train and test dataset\n",
    "        \n",
    "        self.loader_target_tr = loader_target_train\n",
    "        self.loader_target_te= loader_target_test\n",
    "\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "    \n",
    "        \n",
    "        #Latent dim for AE/VAE\n",
    "        self.latent_dim = 256 #25\n",
    "        \n",
    "        \n",
    "        self.generator = GIN \n",
    "        self.epochs = epochs \n",
    "        \n",
    "       \n",
    "        #Classifier\n",
    "        \n",
    "        class_input = Input(shape=(256,))\n",
    "        #x = latent(class_input)\n",
    "        x= Dense(256, activation = \"relu\")(class_input)\n",
    "        class_output = Dense(self.n_classes, activation = \"softmax\")(x)\n",
    "        \n",
    "        self.classifier = Model(class_input, class_output, name=\"classifier\")\n",
    "        \n",
    "        #Discriminator\n",
    "        \n",
    "        disc_input = Input(shape=(256,))\n",
    "        #x = latent(disc_input)\n",
    "        x = Dense(256, activation = \"relu\")(disc_input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dense(256, activation = \"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        disc_output = Dense(2, activation = \"softmax\")(x)\n",
    "        \n",
    "        self.discriminator = Model(disc_input, disc_output, name=\"discriminator\")\n",
    "\n",
    "      \n",
    "        \n",
    "      \n",
    "        self.loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "        \n",
    "        self.lr = 0.001 \n",
    "        self.momentum = 0.9\n",
    "        self.alpha = 0.0002\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.task_optimizer=  Adam(1e-3)\n",
    "        self.gen_optimizer = Adam(1e-3)\n",
    "        self.disc_optimizer = Adam(1e-3)\n",
    "        \n",
    "        self.train_task_loss = tf.keras.metrics.Mean()\n",
    "        self.train_disc_loss = tf.keras.metrics.Mean()\n",
    "        self.train_gen_loss = tf.keras.metrics.Mean()\n",
    "        self.train_task_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "        self.train_target_task_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "   \n",
    "        \n",
    "        self.test_target_task_loss = tf.keras.metrics.Mean()\n",
    "        self.test_target_task_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        self.batch_size = 16\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def train_batch(self, x_source_train, y_source_train, x_target_train, y_target_train, epoch):\n",
    "        \n",
    "        \n",
    "        source = np.tile([1,0], (y_source_train.shape[0], 1))\n",
    "        target = np.tile([0,1], (y_target_train.shape[0], 1))\n",
    "        \n",
    "        target_fake = np.tile([0,1], (y_source_train.shape[0], 1))\n",
    "        source_fake = np.tile([1,0], (y_target_train.shape[0], 1))\n",
    "        \n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            y_domain_pred_source = self.discriminator(self.generator(x_source_train, training=True), training=True)\n",
    "            y_domain_pred_target = self.discriminator(self.generator(x_target_train, training=True), training=True)\n",
    "            \n",
    "            disc_loss = self.loss(source, y_domain_pred_source) +  self.loss(target, y_domain_pred_target)  \n",
    "            \n",
    "        disc_grad = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)  \n",
    "        self.disc_optimizer.apply_gradients(zip(disc_grad, self.discriminator.trainable_variables))\n",
    "        self.train_disc_loss(disc_loss)\n",
    "\n",
    "        with tf.GradientTape() as task_tape, tf.GradientTape() as gen_tape:\n",
    "            \n",
    "            #Forward pass\n",
    "            y_class_pred_source = self.classifier(self.generator(x_source_train, training=True), training=True)\n",
    "            y_class_pred_target = self.classifier(self.generator(x_target_train, training=True), training=True)\n",
    "            y_domain_pred_source = self.discriminator(self.generator(x_source_train, training=True), training=True)\n",
    "            y_domain_pred_target = self.discriminator(self.generator(x_target_train, training=True), training=True)\n",
    "            \n",
    "            \n",
    "            task_loss = self.loss(y_target_train, y_class_pred_target) + 0.1*self.loss(y_source_train, y_class_pred_source)  \n",
    "            adv_loss = self.loss(target_fake, y_domain_pred_source) +  self.loss(source_fake, y_domain_pred_target)   \n",
    "            gen_loss = task_loss +  adv_loss*0.1\n",
    "            \n",
    "        \n",
    "        \n",
    "        # Compute gradients   \n",
    "        task_grad = task_tape.gradient(task_loss, self.classifier.trainable_variables)\n",
    "        gen_grad = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "       \n",
    "        \n",
    "        # Update weights \n",
    "        self.task_optimizer.apply_gradients(zip(task_grad, self.classifier.trainable_variables))\n",
    "        self.gen_optimizer.apply_gradients(zip(gen_grad, self.generator.trainable_variables)) \n",
    "        \n",
    "            \n",
    "\n",
    "        self.train_task_loss(task_loss)\n",
    "        self.train_task_accuracy(y_source_train, y_class_pred_source)\n",
    "        self.train_target_task_accuracy(y_target_train, y_class_pred_target)\n",
    "        self.train_gen_loss(gen_loss)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def test_batch(self, x_target_test, y_target_test):\n",
    "       \n",
    "        # y_class_pred = self.classifier(self.generator(x_source_test, training=False), training=False)\n",
    "        y_target_class_pred = self.classifier(self.generator(x_target_test, training=False), training=False)\n",
    "        \n",
    "            \n",
    "        #self.test_task_loss.update_state(y_source_test, y_class_pred)\n",
    "        self.test_target_task_loss(y_target_test, y_target_class_pred)\n",
    "        #self.test_task_accuracy.update_state(y_source_test, y_class_pred)\n",
    "        self.test_target_task_accuracy(y_target_test, y_target_class_pred)\n",
    "        #self.test_target_f1_score.update_state(y_target_test, y_target_class_pred)\n",
    "        \n",
    "       \n",
    "        \n",
    "        return \n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        output = []\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "        step = 0\n",
    "        while step < loader.steps_per_epoch:\n",
    "            step += 1\n",
    "            inputs, target = loader.__next__()\n",
    "            pred = self.classifier(self.generator(inputs, training=False), training=False)\n",
    "            outs = (\n",
    "                loss_fn(target, pred),\n",
    "                tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "                len(target),  # Keep track of batch size\n",
    "            )          \n",
    "            output.append(outs)\n",
    "            if step == loader.steps_per_epoch:\n",
    "                output = np.array(output)\n",
    "                return np.average(output[:, :-1], 0, weights=output[:, -1])\n",
    "\n",
    "      \n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "    def log_train(self):\n",
    "        \n",
    "        \n",
    "        log_format = 'C_loss train: {:.4f}, Acc train source: {:.2f} , Acc train target: {:.2f}\\n'+'D_loss train: {:.4f}, G_loss train: {:.4f}'\n",
    "\n",
    "        message = log_format.format(\n",
    "                 self.train_task_loss.result(),\n",
    "                 self.train_task_accuracy.result()*100,\n",
    "                 self.train_target_task_accuracy.result()*100,\n",
    "                 self.train_disc_loss.result(),\n",
    "                 self.train_gen_loss.result())\n",
    "        \n",
    "\n",
    "        self.reset_metrics('train')\n",
    "        \n",
    "\n",
    "\n",
    "        return message \n",
    "    \n",
    "    def log_test(self):\n",
    "        \n",
    "        \n",
    "        log_format = \"C_loss test target: {:.4f}, Acc test target: {:.2f}\"\n",
    "\n",
    "        message = log_format.format(\n",
    "                 #self.test_task_loss.result(),\n",
    "                 #self.test_task_accuracy.result()*100,\n",
    "                 self.test_target_task_loss.result(),\n",
    "                 self.test_target_task_accuracy.result()*100)\n",
    "                 #self.test_target_f1_score.result()*100)\n",
    "        \n",
    "\n",
    "        self.reset_metrics('test')\n",
    "\n",
    "\n",
    "        return message \n",
    "    \n",
    "    def reset_metrics(self, target):\n",
    "\n",
    "        if target == 'train':\n",
    "            self.train_task_loss.reset_states()\n",
    "            self.train_task_accuracy.reset_states()\n",
    "            self.train_disc_loss.reset_states()\n",
    "            self.train_gen_loss.reset_states()\n",
    "            \n",
    "        \n",
    "        \n",
    "        if target == 'test':\n",
    "            self.test_target_task_loss.reset_states()\n",
    "            self.test_target_task_accuracy.reset_states()\n",
    "        \n",
    "\n",
    "        return \n",
    "    \n",
    "    def train(self):\n",
    "        epoch = step = 0\n",
    "    \n",
    "\n",
    "        for (source_batch, source_labels), (target_batch, target_labels) in zip(self.loader_source_tr, self.loader_target_tr):\n",
    "            step +=1 \n",
    "            self.train_batch(source_batch, source_labels, target_batch, target_labels, epoch)\n",
    "            if step == min(self.loader_source_tr.steps_per_epoch, self.loader_target_tr.steps_per_epoch):\n",
    "                step = 0\n",
    "                epoch +=1  \n",
    "                if epoch % 10 ==0:\n",
    "                    print('Epoch: {}'.format(epoch))\n",
    "                    print(self.log_train())                 \n",
    "                    results_te = self.evaluate(self.loader_target_te)\n",
    "                    print(\"Test results - Loss: {:.3f} - Acc: {:.3f}\".format(*results_te))\n",
    "                    \n",
    "                    \n",
    "\n",
    "        return self.generator, self.classifier\n",
    "                \n",
    "\n",
    "\n",
    "        \n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9490dad4-fa4e-4707-9d38-bd903b7323b8",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0eb5ff-8095-4f1f-b778-4107c008f675",
   "metadata": {},
   "source": [
    "## Set ramnit as the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74621763-b76d-427e-8e41-70d59a74d5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target malware size: 1503\n",
      "Target normal size: 827\n",
      "Source malware size: 9208\n",
      "Source normal size: 6354\n"
     ]
    }
   ],
   "source": [
    "# ramnit label is 0\n",
    "target_malware, source_malware = filter_label(malware, [0])\n",
    "source_normal = normal \n",
    "target_normal = normal_PE\n",
    "\n",
    "# convert it to binary labels\n",
    "target_malware = binary_label(target_malware, True)\n",
    "source_malware  = binary_label(source_malware, True)\n",
    "target_normal = binary_label(target_normal, False)\n",
    "source_normal = binary_label(source_normal, False)\n",
    "\n",
    "print(\"Target malware size: {}\".format(len(target_malware)))\n",
    "print(\"Target normal size: {}\".format(len(target_normal)))\n",
    "print(\"Source malware size: {}\".format(len(source_malware)))\n",
    "print(\"Source normal size: {}\".format(len(source_normal)))\n",
    "           \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c87ed6d2-5154-4a44-ad37-de32a81cf3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751\n",
      "752\n",
      "413\n",
      "414\n"
     ]
    }
   ],
   "source": [
    "# Split the  malware/benign dataset into train and test set\n",
    "target_malware_train, target_malware_test = train_test_split(target_malware, 0.5)\n",
    "print(len(target_malware_train))\n",
    "print(len(target_malware_test))\n",
    "\n",
    "target_normal_train, target_normal_test = train_test_split(target_normal, 0.5)\n",
    "print(len(target_normal_train))\n",
    "print(len(target_normal_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e20ab23-7e36-4c6a-b4b7-0082e12e5d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target train dataset size: 1164\n",
      "Target test dataset size: 1166\n",
      "Source train dataset size: 11671\n",
      "Source test dataset size: 3891\n"
     ]
    }
   ],
   "source": [
    "#combine the normal and malware dataset\n",
    "source = merge_dataset(source_normal, source_malware)\n",
    "\n",
    "#source dataset train and test split\n",
    "source_train, source_test = train_test_split(source, 0.75)\n",
    "\n",
    "# we do the same for the target dataset\n",
    "target_train = merge_dataset(target_normal_train, target_malware_train)\n",
    "target_test = merge_dataset(target_normal_test, target_malware_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Target train dataset size: {}\".format(len(target_train)))\n",
    "print(\"Target test dataset size: {}\".format(len(target_test)))\n",
    "print(\"Source train dataset size: {}\".format(len(source_train)))\n",
    "print(\"Source test dataset size: {}\".format((len(source_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7c538bc-6ec4-4188-9939-f414c4a6cb62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Sample size 20-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 12:45:03.097788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-26 12:45:03.141628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-26 12:45:03.142620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-26 12:45:03.143897: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-26 12:45:03.144314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-26 12:45:03.145211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-26 12:45:03.146082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-26 12:45:03.471881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-26 12:45:03.472933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-26 12:45:03.473809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-26 12:45:03.474629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21440 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-09-26 12:45:04.643807: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "C_loss train: 0.2157, Acc train source: 86.25 , Acc train target: 94.50\n",
      "D_loss train: 1.6222, G_loss train: 0.4020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 12:45:13.909505: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 69568512 exceeds 10% of free system memory.\n",
      "2024-09-26 12:45:14.050949: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 71660032 exceeds 10% of free system memory.\n",
      "2024-09-26 12:45:14.421192: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 59549184 exceeds 10% of free system memory.\n",
      "2024-09-26 12:45:14.736915: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 68727296 exceeds 10% of free system memory.\n",
      "2024-09-26 12:45:15.223465: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 58120704 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results - Loss: 0.042 - Acc: 0.990\n",
      "Epoch: 20\n",
      "C_loss train: 0.1817, Acc train source: 91.56 , Acc train target: 95.75\n",
      "D_loss train: 1.4250, G_loss train: 0.3583\n",
      "Test results - Loss: 1.267 - Acc: 0.862\n",
      "Done. Test f1: 0.8457292864830777\n",
      "--------------------------Sample size 50-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.2467, Acc train source: 94.53 , Acc train target: 94.80\n",
      "D_loss train: 1.4464, G_loss train: 0.4274\n",
      "Test results - Loss: 0.143 - Acc: 0.936\n",
      "Epoch: 20\n",
      "C_loss train: 0.0881, Acc train source: 95.47 , Acc train target: 96.90\n",
      "D_loss train: 1.2573, G_loss train: 0.2793\n",
      "Test results - Loss: 0.556 - Acc: 0.930\n",
      "Done. Test f1: 0.9245891698313864\n",
      "--------------------------Sample size 100-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.1062, Acc train source: 92.95 , Acc train target: 97.90\n",
      "D_loss train: 1.4298, G_loss train: 0.2790\n",
      "Test results - Loss: 0.133 - Acc: 0.951\n",
      "Epoch: 20\n",
      "C_loss train: 0.0277, Acc train source: 96.88 , Acc train target: 98.80\n",
      "D_loss train: 1.2541, G_loss train: 0.2087\n",
      "Test results - Loss: 0.516 - Acc: 0.799\n",
      "Done. Test f1: 0.7678073420641444\n",
      "--------------------------Sample size 200-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.0861, Acc train source: 93.80 , Acc train target: 97.90\n",
      "D_loss train: 1.3648, G_loss train: 0.2634\n",
      "Test results - Loss: 0.124 - Acc: 0.942\n",
      "Epoch: 20\n",
      "C_loss train: 0.0349, Acc train source: 96.83 , Acc train target: 98.60\n",
      "D_loss train: 1.2533, G_loss train: 0.2127\n",
      "Test results - Loss: 0.045 - Acc: 0.992\n",
      "Done. Test f1: 0.9920731460955514\n",
      "--------------------------Sample size 300-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.0907, Acc train source: 92.34 , Acc train target: 98.20\n",
      "D_loss train: 1.2465, G_loss train: 0.2848\n",
      "Test results - Loss: 0.278 - Acc: 0.859\n",
      "Epoch: 20\n",
      "C_loss train: 0.0400, Acc train source: 96.25 , Acc train target: 98.78\n",
      "D_loss train: 1.1864, G_loss train: 0.2413\n",
      "Test results - Loss: 0.013 - Acc: 0.997\n",
      "Done. Test f1: 0.9973804342878517\n",
      "--------------------------Sample size 500-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.0896, Acc train source: 94.69 , Acc train target: 98.04\n",
      "D_loss train: 1.2587, G_loss train: 0.2744\n",
      "Test results - Loss: 0.029 - Acc: 0.993\n",
      "Epoch: 20\n",
      "C_loss train: 0.0361, Acc train source: 98.05 , Acc train target: 98.77\n",
      "D_loss train: 1.2706, G_loss train: 0.2090\n",
      "Test results - Loss: 0.025 - Acc: 0.991\n",
      "Done. Test f1: 0.9908163352183454\n"
     ]
    }
   ],
   "source": [
    "samples = [20, 50, 100, 200, 300, 500]\n",
    "\n",
    "channels = 256  # Hidden units\n",
    "layers = 3  # GIN layers\n",
    "# We have limited the number of training epochs to 20 to minimize training time.\n",
    "# You can change the epochs to a lower number (lowest 1) just to test if the code is functional.\n",
    "# However, for better model performance, consider increasing the number of epochs to those specified in the referenced paper\n",
    "epochs = 20  \n",
    "batch_size = 16  # Batch size\n",
    "n_out = target_train.n_labels\n",
    "\n",
    "for size in samples: \n",
    "    print(\"--------------------------Sample size {}-------------------------\".format(size))\n",
    "\n",
    "    target_train_select, target_train_re = subsample(target_train, size)\n",
    "\n",
    "    loader_source_tr = DisjointLoader(source_train, batch_size=batch_size, epochs = epochs, shuffle = True)\n",
    "    loader_source_te = DisjointLoader(source_test,  batch_size=batch_size)\n",
    "\n",
    "\n",
    "    loader_target_tr = DisjointLoader(target_train_select, batch_size=batch_size, epochs = epochs, shuffle = True)\n",
    "    loader_target_te = DisjointLoader(target_test, batch_size=batch_size)\n",
    "\n",
    "    # build model\n",
    "    GIN = GIN0(channels, layers)\n",
    "\n",
    "\n",
    "    model =DANN_GIN(loader_source_tr, loader_target_tr, loader_target_te, GIN, n_out, epochs==20) # change the epochs here as well\n",
    "\n",
    "\n",
    "    G, C = model.train()\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    # Evaluate model\n",
    "    ################################################################################\n",
    "    results = []\n",
    "    step = 0\n",
    "\n",
    "    while step < loader_target_te.steps_per_epoch:      \n",
    "        step += 1\n",
    "        inputs, target = loader_target_te.__next__()\n",
    "        pred = C(G(inputs, training=False), training=False)\n",
    "        results.append(\n",
    "            (\n",
    "               f1_score(np.argmax(target, axis=1), np.argmax(pred, axis=1), average='weighted')\n",
    "            )\n",
    "        )\n",
    "    print(\"Done. Test f1: {}\".format(np.mean(results, 0)))\n",
    "\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20924f-6f67-43bc-b90e-b8b0f5e5d393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b73d8f80-17c2-447a-a9a2-f9d11bd11c9c",
   "metadata": {},
   "source": [
    "## Set lollipop as the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eee79dc0-8d4a-4de4-a282-57a6c737449e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target malware size: 2476\n",
      "Target normal size: 827\n",
      "Source malware size: 8235\n",
      "Source normal size: 6354\n"
     ]
    }
   ],
   "source": [
    "# lollipop label is 1\n",
    "target_malware, source_malware = filter_label(malware, [1])\n",
    "source_normal = normal \n",
    "target_normal = normal_PE\n",
    "\n",
    "# convert it to binary labels\n",
    "target_malware = binary_label(target_malware, True)\n",
    "source_malware  = binary_label(source_malware, True)\n",
    "target_normal = binary_label(target_normal, False)\n",
    "source_normal = binary_label(source_normal, False)\n",
    "\n",
    "print(\"Target malware size: {}\".format(len(target_malware)))\n",
    "print(\"Target normal size: {}\".format(len(target_normal)))\n",
    "print(\"Source malware size: {}\".format(len(source_malware)))\n",
    "print(\"Source normal size: {}\".format(len(source_normal)))\n",
    "           \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcd51fe5-4dc4-4fd3-9a63-e847e67ebc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1238\n",
      "1238\n",
      "413\n",
      "414\n"
     ]
    }
   ],
   "source": [
    "# Split the  malware/benign dataset into train and test set\n",
    "target_malware_train, target_malware_test = train_test_split(target_malware, 0.5)\n",
    "print(len(target_malware_train))\n",
    "print(len(target_malware_test))\n",
    "\n",
    "target_normal_train, target_normal_test = train_test_split(target_normal, 0.5)\n",
    "print(len(target_normal_train))\n",
    "print(len(target_normal_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac44864c-8189-4e51-b7d4-03eb5ca4ace9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target train dataset size: 1651\n",
      "Target test dataset size: 1652\n",
      "Source train dataset size: 10941\n",
      "Source test dataset size: 3648\n"
     ]
    }
   ],
   "source": [
    "# combine the normal and malware dataset\n",
    "source = merge_dataset(source_normal, source_malware)\n",
    "\n",
    "# source dataset train and test split\n",
    "source_train, source_test = train_test_split(source, 0.75)\n",
    "# we do the same for the target dataset\n",
    "target_train = merge_dataset(target_normal_train, target_malware_train)\n",
    "target_test = merge_dataset(target_normal_test, target_malware_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Target train dataset size: {}\".format(len(target_train)))\n",
    "print(\"Target test dataset size: {}\".format(len(target_test)))\n",
    "print(\"Source train dataset size: {}\".format(len(source_train)))\n",
    "print(\"Source test dataset size: {}\".format((len(source_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc82f598-bd70-43e7-865f-0550971c8ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Sample size 20-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.2294, Acc train source: 82.81 , Acc train target: 94.50\n",
      "D_loss train: 1.6878, G_loss train: 0.4025\n",
      "Test results - Loss: 0.016 - Acc: 0.999\n",
      "Epoch: 20\n",
      "C_loss train: 0.0623, Acc train source: 89.38 , Acc train target: 97.25\n",
      "D_loss train: 1.4309, G_loss train: 0.2331\n",
      "Test results - Loss: 0.074 - Acc: 0.979\n",
      "Done. Test f1: 0.9778912009588383\n",
      "--------------------------Sample size 50-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.2670, Acc train source: 87.03 , Acc train target: 94.40\n",
      "D_loss train: 1.5278, G_loss train: 0.4388\n",
      "Test results - Loss: 0.174 - Acc: 0.921\n",
      "Epoch: 20\n",
      "C_loss train: 0.0695, Acc train source: 94.84 , Acc train target: 96.60\n",
      "D_loss train: 1.3808, G_loss train: 0.2431\n",
      "Test results - Loss: 0.004 - Acc: 0.999\n",
      "Done. Test f1: 0.9993092396109637\n",
      "--------------------------Sample size 100-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.1523, Acc train source: 94.20 , Acc train target: 95.60\n",
      "D_loss train: 1.4677, G_loss train: 0.3174\n",
      "Test results - Loss: 0.316 - Acc: 0.916\n",
      "Epoch: 20\n",
      "C_loss train: 0.0877, Acc train source: 94.91 , Acc train target: 96.95\n",
      "D_loss train: 1.3479, G_loss train: 0.2500\n",
      "Test results - Loss: 0.281 - Acc: 0.877\n",
      "Done. Test f1: 0.8542483247951408\n",
      "--------------------------Sample size 200-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.0784, Acc train source: 95.67 , Acc train target: 98.30\n",
      "D_loss train: 1.4315, G_loss train: 0.2395\n",
      "Test results - Loss: 0.018 - Acc: 0.996\n",
      "Epoch: 20\n",
      "C_loss train: 0.0295, Acc train source: 97.98 , Acc train target: 98.75\n",
      "D_loss train: 1.3242, G_loss train: 0.1884\n",
      "Test results - Loss: 0.038 - Acc: 0.986\n",
      "Done. Test f1: 0.9855836911679302\n",
      "--------------------------Sample size 300-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.0701, Acc train source: 95.89 , Acc train target: 97.83\n",
      "D_loss train: 1.4052, G_loss train: 0.2268\n",
      "Test results - Loss: 0.006 - Acc: 0.998\n",
      "Epoch: 20\n",
      "C_loss train: 0.0165, Acc train source: 98.52 , Acc train target: 98.68\n",
      "D_loss train: 1.3370, G_loss train: 0.1697\n",
      "Test results - Loss: 0.000 - Acc: 1.000\n",
      "Done. Test f1: 1.0\n",
      "--------------------------Sample size 500-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.0723, Acc train source: 95.90 , Acc train target: 98.06\n",
      "D_loss train: 1.3675, G_loss train: 0.2365\n",
      "Test results - Loss: 0.087 - Acc: 0.966\n",
      "Epoch: 20\n",
      "C_loss train: 0.0223, Acc train source: 97.54 , Acc train target: 98.80\n",
      "D_loss train: 1.3020, G_loss train: 0.1837\n",
      "Test results - Loss: 0.010 - Acc: 0.998\n",
      "Done. Test f1: 0.9981043956043956\n"
     ]
    }
   ],
   "source": [
    "samples = [20, 50, 100, 200, 300, 500]\n",
    "\n",
    "channels = 256  # Hidden units\n",
    "layers = 3  # GIN layers\n",
    "# We have limited the number of training epochs to 20 to minimize training time.\n",
    "# You can change the epochs to a lower number (lowest 1) just to test if the code is functional.\n",
    "# However, for better model performance, consider increasing the number of epochs to those specified in the referenced paper.\n",
    "epochs = 20  \n",
    "batch_size = 16  # Batch size\n",
    "n_out = target_train.n_labels\n",
    "\n",
    "for size in samples: \n",
    "    print(\"--------------------------Sample size {}-------------------------\".format(size))\n",
    "\n",
    "    target_train_select, target_train_re = subsample(target_train, size)\n",
    "\n",
    "    loader_source_tr = DisjointLoader(source_train, batch_size=batch_size, epochs = epochs, shuffle = True)\n",
    "    loader_source_te = DisjointLoader(source_test,  batch_size=batch_size)\n",
    "\n",
    "\n",
    "    loader_target_tr = DisjointLoader(target_train_select, batch_size=batch_size, epochs = epochs, shuffle = True)\n",
    "    loader_target_te = DisjointLoader(target_test, batch_size=batch_size)\n",
    "\n",
    "    # build model\n",
    "    GIN = GIN0(channels, layers)\n",
    "\n",
    "\n",
    "    model =DANN_GIN(loader_source_tr, loader_target_tr, loader_target_te, GIN, n_out, epochs==20) # change the epochs here as well\n",
    "\n",
    "\n",
    "    G, C = model.train()\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    # Evaluate model\n",
    "    ################################################################################\n",
    "    results = []\n",
    "    step = 0\n",
    "\n",
    "    while step < loader_target_te.steps_per_epoch:      \n",
    "        step += 1\n",
    "        inputs, target = loader_target_te.__next__()\n",
    "        pred = C(G(inputs, training=False), training=False)\n",
    "        results.append(\n",
    "            (\n",
    "               f1_score(np.argmax(target, axis=1), np.argmax(pred, axis=1), average='weighted')\n",
    "            )\n",
    "        )\n",
    "    print(\"Done. Test f1: {}\".format(np.mean(results, 0)))\n",
    "\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c044e-3d3c-4d1b-b208-a18afaa0799b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30c02311-9579-4690-baae-ed63e88678df",
   "metadata": {},
   "source": [
    "## Set kelihos_v3 as the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a973442-806a-416a-baf7-3276e7a59583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target malware size: 2919\n",
      "Target normal size: 827\n",
      "Source malware size: 7792\n",
      "Source normal size: 6354\n"
     ]
    }
   ],
   "source": [
    "# Kelihos_ver3 label is 3\n",
    "target_malware, source_malware = filter_label(malware, [2])\n",
    "source_normal = normal \n",
    "target_normal = normal_PE\n",
    "\n",
    "# convert it to binary labels\n",
    "target_malware = binary_label(target_malware, True)\n",
    "source_malware  = binary_label(source_malware, True)\n",
    "target_normal = binary_label(target_normal, False)\n",
    "source_normal = binary_label(source_normal, False)\n",
    "\n",
    "print(\"Target malware size: {}\".format(len(target_malware)))\n",
    "print(\"Target normal size: {}\".format(len(target_normal)))\n",
    "print(\"Source malware size: {}\".format(len(source_malware)))\n",
    "print(\"Source normal size: {}\".format(len(source_normal)))\n",
    "           \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01793b0e-852a-4f07-b8e9-ad39fa234ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1459\n",
      "1460\n",
      "413\n",
      "414\n"
     ]
    }
   ],
   "source": [
    "# Split the  malware/benign dataset into train and test set\n",
    "target_malware_train, target_malware_test = train_test_split(target_malware, 0.5)\n",
    "print(len(target_malware_train))\n",
    "print(len(target_malware_test))\n",
    "# Split the normal dataset into train and test set\n",
    "target_normal_train, target_normal_test = train_test_split(target_normal, 0.5)\n",
    "print(len(target_normal_train))\n",
    "print(len(target_normal_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8d7077b-4ff1-44bd-ad46-8282b075189f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target train dataset size: 1872\n",
      "Target test dataset size: 1874\n",
      "Source train dataset size: 10609\n",
      "Source test dataset size: 3537\n"
     ]
    }
   ],
   "source": [
    "# combine the normal and malware dataset\n",
    "source = merge_dataset(source_normal, source_malware)\n",
    "\n",
    "# source dataset train and test split\n",
    "source_train, source_test = train_test_split(source, 0.75)\n",
    "# we do the same for the target dataset\n",
    "target_train = merge_dataset(target_normal_train, target_malware_train)\n",
    "target_test = merge_dataset(target_normal_test, target_malware_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Target train dataset size: {}\".format(len(target_train)))\n",
    "print(\"Target test dataset size: {}\".format(len(target_test)))\n",
    "print(\"Source train dataset size: {}\".format(len(source_train)))\n",
    "print(\"Source test dataset size: {}\".format((len(source_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1c736a9-b592-4f22-a402-188097d34f88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Sample size 20-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.2553, Acc train source: 90.00 , Acc train target: 93.50\n",
      "D_loss train: 1.6589, G_loss train: 0.4294\n",
      "Test results - Loss: 0.127 - Acc: 0.933\n",
      "Epoch: 20\n",
      "C_loss train: 0.1244, Acc train source: 93.75 , Acc train target: 95.75\n",
      "D_loss train: 1.4348, G_loss train: 0.2955\n",
      "Test results - Loss: 0.083 - Acc: 0.961\n",
      "Done. Test f1: 0.956974956663944\n",
      "--------------------------Sample size 50-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.3087, Acc train source: 92.19 , Acc train target: 92.80\n",
      "D_loss train: 1.5220, G_loss train: 0.4867\n",
      "Test results - Loss: 0.054 - Acc: 0.999\n",
      "Epoch: 20\n",
      "C_loss train: 0.1863, Acc train source: 95.94 , Acc train target: 95.30\n",
      "D_loss train: 1.3240, G_loss train: 0.3614\n",
      "Test results - Loss: 0.010 - Acc: 0.998\n",
      "Done. Test f1: 0.9983585309174844\n",
      "--------------------------Sample size 100-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.2671, Acc train source: 94.38 , Acc train target: 95.90\n",
      "D_loss train: 1.4819, G_loss train: 0.4367\n",
      "Test results - Loss: 0.026 - Acc: 0.999\n",
      "Epoch: 20\n",
      "C_loss train: 0.1029, Acc train source: 96.61 , Acc train target: 97.10\n",
      "D_loss train: 1.2639, G_loss train: 0.2823\n",
      "Test results - Loss: 0.014 - Acc: 0.997\n",
      "Done. Test f1: 0.9968967253172396\n",
      "--------------------------Sample size 200-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.1251, Acc train source: 95.62 , Acc train target: 97.10\n",
      "D_loss train: 1.4062, G_loss train: 0.3019\n",
      "Test results - Loss: 0.020 - Acc: 0.997\n",
      "Epoch: 20\n",
      "C_loss train: 0.0745, Acc train source: 96.78 , Acc train target: 98.03\n",
      "D_loss train: 1.2343, G_loss train: 0.2595\n",
      "Test results - Loss: 0.022 - Acc: 0.997\n",
      "Done. Test f1: 0.9972246821657286\n",
      "--------------------------Sample size 300-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.1390, Acc train source: 93.22 , Acc train target: 96.73\n",
      "D_loss train: 1.3868, G_loss train: 0.3077\n",
      "Test results - Loss: 0.013 - Acc: 1.000\n",
      "Epoch: 20\n",
      "C_loss train: 0.0443, Acc train source: 96.22 , Acc train target: 97.98\n",
      "D_loss train: 1.1931, G_loss train: 0.2298\n",
      "Test results - Loss: 0.002 - Acc: 1.000\n",
      "Done. Test f1: 1.0\n",
      "--------------------------Sample size 500-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.1263, Acc train source: 94.10 , Acc train target: 96.68\n",
      "D_loss train: 1.2807, G_loss train: 0.3089\n",
      "Test results - Loss: 0.002 - Acc: 0.999\n",
      "Epoch: 20\n",
      "C_loss train: 0.0657, Acc train source: 96.21 , Acc train target: 97.78\n",
      "D_loss train: 1.1324, G_loss train: 0.2716\n",
      "Test results - Loss: 0.039 - Acc: 0.987\n",
      "Done. Test f1: 0.9873114882434106\n"
     ]
    }
   ],
   "source": [
    "samples = [20, 50, 100, 200, 300, 500]\n",
    "\n",
    "channels = 256  # Hidden units\n",
    "layers = 3  # GIN layers\n",
    "# We have limited the number of training epochs to 20 to minimize training time.\n",
    "# You can change the epochs to a lower number (lowest 1) just to test if the code is functional.\n",
    "# However, for better model performance, consider increasing the number of epochs to those specified in the referenced paper\n",
    "epochs = 20  \n",
    "batch_size = 16  # Batch size\n",
    "n_out = target_train.n_labels\n",
    "\n",
    "for size in samples: \n",
    "    print(\"--------------------------Sample size {}-------------------------\".format(size))\n",
    "    \n",
    "    target_train_select, target_train_re = subsample(target_train, size)\n",
    "\n",
    "    loader_source_tr = DisjointLoader(source_train, batch_size=batch_size, epochs = epochs, shuffle = True)\n",
    "    loader_source_te = DisjointLoader(source_test,  batch_size=batch_size)\n",
    "\n",
    "\n",
    "    loader_target_tr = DisjointLoader(target_train_select, batch_size=batch_size, epochs = epochs, shuffle = True)\n",
    "    loader_target_te = DisjointLoader(target_test, batch_size=batch_size)\n",
    "\n",
    "    # build model\n",
    "    GIN = GIN0(channels, layers)\n",
    "\n",
    "\n",
    "    model =DANN_GIN(loader_source_tr, loader_target_tr, loader_target_te, GIN, n_out, epochs==20) # change the epochs here as well\n",
    "\n",
    "\n",
    "    G, C = model.train()\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    # Evaluate model\n",
    "    ################################################################################\n",
    "    results = []\n",
    "    step = 0\n",
    "\n",
    "    while step < loader_target_te.steps_per_epoch:      \n",
    "        step += 1\n",
    "        inputs, target = loader_target_te.__next__()\n",
    "        pred = C(G(inputs, training=False), training=False)\n",
    "        results.append(\n",
    "            (\n",
    "               f1_score(np.argmax(target, axis=1), np.argmax(pred, axis=1), average='weighted')\n",
    "            )\n",
    "        )\n",
    "    print(\"Done. Test f1: {}\".format(np.mean(results, 0)))\n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2856247a-a5e5-4a87-85b2-b432f2b4b0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ipykernel",
   "language": "python",
   "name": "ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
