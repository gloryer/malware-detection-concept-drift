import os
import json
import csv
import json
import re
import sys

import numpy as np
import scipy.sparse as sp
import pandas as pd 
from torch.utils.data import DataLoader

import networkx as nx

from scipy.sparse import csr_matrix
from bert_data import REGS, TYPES
from constant import *

sys.path.append('Palmtree/pre-trained_model')

import eval_utils as utils


class CFG_Normalized(object):
    def __init__(self, root,label_path, store_path, encoder):
        self.root = root
        self.label_path = label_path
        self.store_path = store_path
        self.encoder = encoder
        
    
    
    def parse_instruction(self, ins):
        ''' every token is seperated by space '''
        ins = re.sub('\s+', ',', ins, 1)
        parts = ins.split(',')
        operand = []
        if len(parts) > 1:
            operand = parts[1:]
        for i in range(len(operand)):
            #print(operand)
            symbols = re.split('([0-9A-Za-z]+)', operand[i])  
            #print(symbols)
            symbols = ' '.join(symbols).split()
            operand[i] = " ".join(symbols)
        opcode = parts[0]
        
        return " ".join([opcode]+operand)
    
    def normalize(self, opcode_operands):
        ''' original normalization function, seperate tokens by _ '''
        if opcode_operands[0] == 'call':
            return 'call'

        ret_ins_str = opcode_operands[0]

        for operand in opcode_operands[1:]:

            if operand in REGS:
                ret_ins_str += '_{}'.format(operand)
            elif operand.startswith('[') and operand.endswith(']'):
                ret_ins_str += '_{}'.format(self._handle_ptr(operand))
            elif operand.startswith('ds:') or '_' in operand:
                ret_ins_str += '_MEM'
            elif operand.isnumeric() or operand.endswith('h'):
                ret_ins_str += '_IMM'
            elif operand in TYPES:
                continue
            else:
                ret_ins_str += '_{}'.format(operand)
        
        return ret_ins_str

    def normalize_comma(self, opcode_operands):
        ''' instruction normalization required by Palm Tree '''
        
        if opcode_operands[0] == 'call':
            return 'call'

        ret_ins_str = opcode_operands[0]

        for operand in opcode_operands[1:]:

            if operand in REGS:
                ret_ins_str += ',{}'.format(operand)
            elif operand.startswith('[') and operand.endswith(']'):
                
                ret_ins_str += ',{}'.format(self._handle_ptr(operand))
            # elif operand.startswith('ds:') or '_' in operand:
            #     ret_ins_str += ',MEM'
            elif operand.isnumeric() or operand.endswith('h'):
                ret_ins_str += ',addr'
            elif operand in TYPES:
                continue
            else:
                #ret_ins_str += ',{}'.format(operand)
                 ret_ins_str += ',string'
        
        return ret_ins_str

    def _handle_ptr(self, ptr):
        ''' 
        [ebp-1Ch] [ebp+8+4] [esp+40h+-18h] [ebp+esp*4] [ebp+8]
        '''

        def _judge_num(string):
            try:
                if string.endswith('h'):
                    tmp = int(string[:-1], 16)
                    return True
                else:
                    return False
            except:
                return False

        # print(ptr, end='\t')
        ptr = ptr.replace('+-', '-')

        ret_ptr = '['
        item = ''
        count = 0
        operator = ''

        for char in ptr[1:]:
            if char in ['+', '-', ']']:
                if not item.isnumeric() and not _judge_num(item):
                    ret_ptr += operator + item
                else:
                    if item.isnumeric():
                        value = int(item)
                    else:
                        value = int('0x'+item[:-1], 16)

                    if operator == '+':
                        count += value
                    elif operator == '-':
                        count -= value
                operator = char if char != ']' else ''
                item = ''
            else:
                item += char
        
        # print(count, end='\t')
        if count <= -10:
            ret_ptr += '-' + (hex(count)[3:]).upper() + 'h]'
        elif -10 < count < 0:
            ret_ptr += '-' + (hex(count)[3:]).upper() + ']'
        elif count == 0:
            ret_ptr += ']'
        elif 0 < count < 10:
            ret_ptr += '+' + (hex(count)[2:]).upper() + ']'
        elif count >= 10:
            ret_ptr += '+' + (hex(count)[2:]).upper() + 'h]'
        
        # print(ret_ptr)
        return ret_ptr

    def process(self):
        ''' process raw JSON CFGs '''
        

        # labels
        labels = {}
        label_set = list()
        with open(self.label_path, 'r') as f:
            data = csv.reader(f)
            for row in data:
                if row[0] == 'Id':
                    continue
                labels[row[0]] = int(row[1])
                if row[1] not in label_set:
                    label_set.append(row[1])
                    
        num_classes = len(label_set)
                    
        file_list = os.listdir(self.root)
        file_list = list(filter(lambda x: '.json' in x, file_list))
        
        idx = 0
        err_dict ={}
        err_dict["err_num"] = 0
        err_dict["err_files"] = []
        for filepath in file_list:


            fullpath = os.path.join(self.root, filepath)
            with open(fullpath, 'r') as f:
                try: 
                    cfg = json.load(f)
                except ValueError as e:
                    print("processed graph {} from file {}".format(idx, filepath))
                    print("invalid json file")
                    err_dict["err_num"]+=1
                    err_dict["err_files"].append(filepath)
                    continue
                else:
                    pass

            ## y (label)
            y = np.zeros((num_classes,))
            y[int(labels[filepath.split('.')[0]]) - 1] = 1


            #get node attribute matrix 

            addr_to_id = dict() # {str: int}
            current_node_id = -1


            x = list() # node attributes
            block_id = 0

            for addr, block in cfg.items(): # addr is 'str
                current_node_id += 1
                addr_to_id[addr] = current_node_id

                # get tokenized opcode sequence as node attributes
                tokenized_seq = []
                embeddings = []
                for insn in block['insn_list']:
                    #print(json.dumps(insn, indent=4))
                    
                    opcode = insn['opcode']
                    operands = insn['operands']

                    opcode_operands = [opcode] + operands
                    #normalize instructions and seperate opcode and oprand with commas
                    normalized = self.normalize_comma(opcode_operands)
                    #parse the instructions to be used as the inputs for PalmTree model
                    tokenized = self.parse_instruction(normalized)
                   
                    
                    tokenized_seq.append(tokenized)
                    
                    
                    del tokenized
                    
                
                #generate embeddings for each instruction sequence in the basic block
               
                #palmtree = utils.UsableTransformer(model_path="/home/li3944/Cisco/PalmTree/pre-trained_model/palmtree/transformer.ep19", vocab_path="/home/li3944/Cisco/PalmTree/pre-trained_model/palmtree/vocab")
                sequence_loader = DataLoader(tokenized_seq, batch_size=500, shuffle=False)
                
                for i, batch in enumerate(sequence_loader):
                    #print("the {} batch is \n".format(i))
                    #print(batch)
                    
                
                    batch_embeddings = self.encoder.encode(batch) 
                    #batch_embeddings  = np.reshape(batch_embeddings , (batch_embeddings .shape[1])) 
                    if i < 1: 
                        embeddings.append(batch_embeddings)
                        embeddings = np.array(embeddings)
                        embeddings = np.reshape(embeddings, (embeddings.shape[1],embeddings.shape[2])) 
             
                    else: 
                        embeddings = np.append(embeddings,batch_embeddings,axis = 0)
                    
  
                block_embeddings = np.mean(embeddings, 0, keepdims = True)
                #block_embeddings = np.reshape(block_embeddings, (block_embeddings.shape[1])) 
                del embeddings

                x.append(block_embeddings)
                #origin_x.append(tokenized_seq)
                #break
                
               
                del block_embeddings
                block_id +=1
            
   
            
                
                
           
            
            
            x_np = np.array(x)
            del x 
            x_np = np.reshape(x_np, (x_np.shape[0], x_np.shape[2])) 
            
            #print(x_np[3])

            # get sparse adjacent matrix
            # node id starts from 0 
            edge_list = list()
            for addr, block in cfg.items(): # addr is `str`
                start_nid = addr_to_id[addr]
                for out_edge in block['out_edge_list']:
                    end_nid = addr_to_id[str(out_edge)]

                    ## edge_index
                    edge_list.append([start_nid, end_nid])

            print("processed graph {} from file {}".format(idx, filepath))
            print("node attribute matrix shape is \n{}".format(x_np.shape))

            node_list = [i for i in range(len(cfg.items()))]
            #print(len(node_list))

            #construct a directed networkx Graph from the edge list 

            G = nx.from_edgelist(edge_list, create_using=nx.DiGraph())

            # construct a sparse adjacency array 
            a = nx.to_scipy_sparse_array(G, nodelist = node_list)

            #print("adjacenct matrix is \n{}".format(a.toarray()))

            # convert it to Compressed Sparse Row matrix
            a = csr_matrix(a)
            #print("Ajacency matrix shape is \n{}".format(x_np.shape))

            #print("label is \n{}".format(y))

            #save to a file
            save_path = 'graph_{}_{}'.format(idx, filepath.split('.')[0])
            save_pat_sparse_matrix = 'graph_{}_{}_sparse_matrix'.format(idx, filepath.split('.')[0])

            filename = os.path.join(self.store_path, save_path)
            filename_sparse_matrix = os.path.join(self.store_path, save_pat_sparse_matrix)
            np.savez(filename, x=x_np, y=y)
            sp.save_npz(filename_sparse_matrix, a)
            
            del x_np, a, y, G, edge_list, node_list
            

            idx += 1
            
        print("Processing completed. \nThere are {} files we couldn't parse".format(err_dict["err_num"]))
            
    def process_malwarebazaaar(self):
        ''' process raw JSON CFGs '''
        

        labels = {}
        label_set = list()
        data_pd = pd.read_csv(self.label_path) 
        with open(self.label_path, 'r') as f:
            data = csv.reader(f)
            for row in data:
                if row[0] == 'Malware SHA-256':
                    continue
                labels[row[0]] = int(row[2])
                if row[2] not in label_set:
                    label_set.append(row[2])
                    
        num_classes = len(label_set)
                    
        file_list = os.listdir(self.root)
        file_list = list(filter(lambda x: '.json' in x, file_list))
        #file_list = file_list[:1]
        
        idx = 0
        #832
        err_dict ={}
        err_dict["err_num"] = 0
        err_dict["err_files"] = []
        for filepath in file_list:


            fullpath = os.path.join(self.root, filepath)
            with open(fullpath, 'r') as f:
                try: 
                    cfg = json.load(f)
                except ValueError as e:
                    print("processed graph {} from file {}".format(idx, filepath))
                    print("invalid json file")
                    err_dict["err_num"]+=1
                    err_dict["err_files"].append(filepath)
                    continue
                else:
                    pass

            ## y (label)
            ## y (label)
            y = np.zeros((num_classes,))
            y[int(labels[filepath.split('.')[0].split("_")[0]])] = 1
            #print(y)
            #print(labels[filepath.split('.')[0].split("_")[0]])
            #print(filepath)
            #print(data_pd.loc[data_pd["Malware SHA-256"] == filepath.split('.')[0].split("_")[0]])



            #get node attribute matrix 

            addr_to_id = dict() # {str: int}
            current_node_id = -1


            x = list() # node attributes
            block_id = 0

            for addr, block in cfg.items(): # addr is 'str
                current_node_id += 1
                addr_to_id[addr] = current_node_id

                # get tokenized opcode sequence as node attributes
                tokenized_seq = []
                embeddings = []
                for insn in block['insn_list']:
                    #print(json.dumps(insn, indent=4))
                    
                    opcode = insn['opcode']
                    operands = insn['operands']

                    opcode_operands = [opcode] + operands
                    #normalize instructions and seperate opcode and oprand with commas
                    normalized = self.normalize_comma(opcode_operands)
                    #parse the instructions to be used as the inputs for PalmTree model
                    tokenized = self.parse_instruction(normalized)
                   
                    
                    tokenized_seq.append(tokenized)
                    
                    
                    del tokenized
                    
                
                #generate embeddings for each instruction sequence in the basic block
               
                #palmtree = utils.UsableTransformer(model_path="/home/li3944/Cisco/PalmTree/pre-trained_model/palmtree/transformer.ep19", vocab_path="/home/li3944/Cisco/PalmTree/pre-trained_model/palmtree/vocab")
                sequence_loader = DataLoader(tokenized_seq, batch_size=500, shuffle=False)
                # if block_id ==3:  
                #     print("printing sequences in BB {}".format(block_id))
                #     print("there are {} instructions in this BB".format(len(tokenized_seq)))
                #     print(tokenized_seq)
                for i, batch in enumerate(sequence_loader):
                    #print("the {} batch is \n".format(i))
                    #print(batch)
                    
                
                    batch_embeddings = self.encoder.encode(batch) 
                    #batch_embeddings  = np.reshape(batch_embeddings , (batch_embeddings .shape[1])) 
                    if i < 1: 
                        embeddings.append(batch_embeddings)
                        embeddings = np.array(embeddings)
                        embeddings = np.reshape(embeddings, (embeddings.shape[1],embeddings.shape[2])) 
             
                    else: 
                        embeddings = np.append(embeddings,batch_embeddings,axis = 0)
                    
                    #embeddings = np.array(embeddings)
                    #if i > 1: 
                    #embeddings = np.reshape(embeddings, (embeddings.shape[1],embeddings.shape[2])) 
                    
               
                    # if block_id ==3:
                    #     print("the {} batch is \n".format(i))
                    #     print(batch)
                    #     print("batch_embeddings shape is {}".format(batch_embeddings.shape))
                    #     print("embeddings shape is {}".format(embeddings.shape))
                    
                # if block_id ==4:    
                #     print(embeddings)
                #embeddings = np.array(embeddings)
                #embeddings = np.reshape(embeddings, (embeddings.shape[1],embeddings.shape[2])) 
             
               
                    #embeddings = self.encoder.encode(tokenized_seq)
                
                #embeddings = embeddings.to('cpu')
                #print("usable embedding of this basicblock:", embeddings)
                #print("the shape of output tensor: ", embeddings.shape)
                #print(embeddings)
                #del tokenized_seq
                
                #tokenized_seq = np.array(tokenized_seq)
                #print(tokenized_seq.shape)
                #print(tokenized_seq)
  
                block_embeddings = np.mean(embeddings, 0, keepdims = True)
                #block_embeddings = np.reshape(block_embeddings, (block_embeddings.shape[1])) 
                del embeddings

                x.append(block_embeddings)
                #origin_x.append(tokenized_seq)
                #break
                
               
                del block_embeddings
                block_id +=1
            
                #gc.collect()
                #torch.cuda.empty_cache()
                #gc.collect()
                #torch.cuda.empty_cache()
                #break
            #print(x[0])
            
            
                
                
           
            
            
            x_np = np.array(x)
            del x 
            x_np = np.reshape(x_np, (x_np.shape[0], x_np.shape[2])) 
            
            #print(x_np[3])

            # get sparse adjacent matrix
            # node id starts from 0 
            edge_list = list()
            for addr, block in cfg.items(): # addr is `str`
                start_nid = addr_to_id[addr]
                for out_edge in block['out_edge_list']:
                    end_nid = addr_to_id[str(out_edge)]

                    ## edge_index
                    edge_list.append([start_nid, end_nid])

            print("processed graph {} from file {}".format(idx, filepath))
            print("node attribute shape is \n{}".format(x_np.shape))
            #print(origin_x[0])
            #print("node attributes for node 0 is \n{}".format(x_np[0]))
            #print("edge list is \n {}".format(edge_list))
            node_list = [i for i in range(len(cfg.items()))]
            #print(len(node_list))

            #construct a directed networkx Graph from the edge list 

            G = nx.from_edgelist(edge_list, create_using=nx.DiGraph())

            # construct a sparse adjacency array 
            a = nx.to_scipy_sparse_array(G, nodelist = node_list)

            #print("adjacenct matrix is \n{}".format(a.toarray()))

            # convert it to Compressed Sparse Row matrix
            a = csr_matrix(a)

            #print("label is \n{}".format(y))

            #save to a file
            save_path = 'graph_{}_{}'.format(idx, filepath.split('.')[0])
            save_pat_sparse_matrix = 'graph_{}_{}_sparse_matrix'.format(idx, filepath.split('.')[0])

            filename = os.path.join(self.store_path, save_path)
            filename_sparse_matrix = os.path.join(self.store_path, save_pat_sparse_matrix)
            np.savez(filename, x=x_np, y=y)
            sp.save_npz(filename_sparse_matrix, a)
            
            del x_np, a, y, G, edge_list, node_list
            
   
            idx += 1
            #break
            # if idx == 2:
            #     break
            #gc.collect()
            #torch.cuda.empty_cache()
            
        print("Processing completed. \nThere are {} files we couldn't parse".format(err_dict["err_num"]))
 




if __name__ == "__main__":
    # The following code generates the embeddings for the raw cfg nodes using the pre-trained PalmTree model.
    # The palmtree project is used to build this code https://github.com/palmtreemodel/PalmTree. 
    # The output of each graph is stored in two files: one for the node attributes (graph_{id}_{name}) and the other for the adjacency matrix (graph_{id}_{name}_sparse_matrix).
    # The outputs are stored in /data/examples/outputs/cfg.
    # We only include one CFG from the Big-15 dataset to show the code is functional. But the code can be used to produce the embeddings for all the cfgs in the path directory.
    path = '../data/examples/inputs/cfg_sample'
    label_path = '../data/labels/trainLabels.csv'
    store_path = '../data/examples/outputs/cfg'

    palmtree = utils.UsableTransformer(model_path="Palmtree/pre-trained_model/palmtree/transformer.ep19", vocab_path="Palmtree/pre-trained_model/palmtree/vocab")


    processor = CFG_Normalized(path, label_path, store_path, palmtree)

    #node id start from 0 
    processor.process()

