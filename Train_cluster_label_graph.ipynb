{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52ba5b24-7613-4ae7-b66c-941d3f5ff941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from spektral.data import Dataset, DisjointLoader, Graph\n",
    "from spektral.layers import GINConv, GlobalAvgPool\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aee62f-71c9-47df-90fa-8e40abb7be02",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Load graph data as spektral graph objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "759ed7f5-4d27-485e-9289-ac8e293b3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load malware data with the cluster labels\n",
    "class GraphData_Malware_New_Label(Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, cfg_path, label_path, **kwargs):\n",
    "        self.cfg_path = cfg_path\n",
    "        self.label_path = label_path\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        \n",
    "        file_list = os.listdir(self.cfg_path)\n",
    "        file_list_x_y = list(filter(lambda x: '_sparse_matrix' not in x and '.npz' in x, file_list))\n",
    "        \n",
    "        #print(len(file_list_x_y))\n",
    "        output = []\n",
    "        label = pd.read_csv(self.label_path, header=0)\n",
    "        \n",
    "        # checklist =[\"bUAOl6u0gwLvqxBHJjeV\",\n",
    "        #            \"4bP0fpWK9ygln6RwLCmY\",\n",
    "        #            \"ej60US1l5DPCKGpvRFyw\"]\n",
    "        \n",
    "       \n",
    "        for filepath in file_list_x_y:\n",
    "\n",
    "            #full path of node attribute and label\n",
    "            fullpath = os.path.join(self.cfg_path, filepath)\n",
    "            #file path of adj matrix\n",
    "            filepath_sp = filepath.split('.')[0] + \"_sparse_matrix.npz\"\n",
    "            #full path pf adj matrix\n",
    "            fullpath_sp = os.path.join(self.cfg_path, filepath_sp)\n",
    "            #with open(fullpath_sp, 'rb') as f1:\n",
    "            sparse_matrix = sp.load_npz(fullpath_sp)\n",
    "            sparse_matrix = sparse_matrix.astype('float32')\n",
    "           \n",
    "            #with open(fullpath, 'rb') as f2:\n",
    "            data = np.load(fullpath)\n",
    "            \n",
    "            #graph_6095_2oUq3FLziRydvHSXM7na.npz\n",
    "            \n",
    "            asm_id = filepath.split('_')[2].split(\".\")[0]\n",
    "            \n",
    "          \n",
    "                \n",
    "            \n",
    "            if asm_id in label[\"asm_id\"].values: \n",
    "                new_y= label.loc[label[\"asm_id\"] == asm_id][\"Class\"].values[0]\n",
    "                \n",
    "            # if asm_id in checklist:\n",
    "            #     print(\"asm_id{}: label{}\".format(asm_id, new_y))\n",
    "                #print(new_y)\n",
    "    \n",
    "            \n",
    "            # Remove diagonal elements\n",
    "            adj = sparse_matrix - sp.dia_matrix((sparse_matrix.diagonal()[np.newaxis, :], [0]), shape=sparse_matrix.shape)\n",
    "            adj.eliminate_zeros()\n",
    "            # Check that diag is zero:\n",
    "            assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "            adj_triu = sp.triu(adj)\n",
    "            adj_tuple = sparse_to_tuple(adj_triu)\n",
    "            edges = adj_tuple[0]\n",
    "            \n",
    "\n",
    "   \n",
    "            \n",
    "            #important! filter out noisy data where the number of basic block is less than 10 and the number of non-self edges in the upper triangle is less than 3\n",
    "            # if the graph is too large, we will run into oom problems\n",
    "            if data[\"x\"].shape[0] >=10 and edges.shape[0] >= 3 and sparse_matrix.shape[0] <=46000:\n",
    "                output.append(Graph(x=data['x'], a= sparse_matrix, y=new_y))\n",
    "                \n",
    "               \n",
    "          \n",
    "          \n",
    "\n",
    "        return output\n",
    "    \n",
    "#load benign data \n",
    "class GraphData(Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, cfg_path, **kwargs):\n",
    "        self.cfg_path = cfg_path\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        \n",
    "        file_list = os.listdir(self.cfg_path)\n",
    "        file_list_x_y = list(filter(lambda x: '_sparse_matrix' not in x and '.npz' in x, file_list))\n",
    "        \n",
    "        #print(len(file_list_x_y))\n",
    "        output = []\n",
    "        \n",
    "        \n",
    "       \n",
    "        for filepath in file_list_x_y:\n",
    "\n",
    "            #full path of node attribute and label\n",
    "            fullpath = os.path.join(self.cfg_path, filepath)\n",
    "            #file path of adj matrix\n",
    "            filepath_sp = filepath.split('.')[0] + \"_sparse_matrix.npz\"\n",
    "            #full path pf adj matrix\n",
    "            fullpath_sp = os.path.join(self.cfg_path, filepath_sp)\n",
    "            #with open(fullpath_sp, 'rb') as f1:\n",
    "            sparse_matrix = sp.load_npz(fullpath_sp)\n",
    "            sparse_matrix = sparse_matrix.astype('float32')\n",
    "           \n",
    "            #with open(fullpath, 'rb') as f2:\n",
    "            data = np.load(fullpath)\n",
    "            \n",
    "            # Remove diagonal elements\n",
    "            adj = sparse_matrix - sp.dia_matrix((sparse_matrix.diagonal()[np.newaxis, :], [0]), shape=sparse_matrix.shape)\n",
    "            adj.eliminate_zeros()\n",
    "            # Check that diag is zero:\n",
    "            assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "            adj_triu = sp.triu(adj)\n",
    "            adj_tuple = sparse_to_tuple(adj_triu)\n",
    "            edges = adj_tuple[0]\n",
    "            \n",
    "                \n",
    "   \n",
    "            \n",
    "            #important! filter out noisy data where the number of basic block is less than 10 and the number of non-self edges in the upper triangle is less than 3\n",
    "            # if the graph is too large, we will run into oom problems\n",
    "            if data[\"x\"].shape[0] >=10 and edges.shape[0] >= 3 and sparse_matrix.shape[0] <=46000:\n",
    "                output.append(Graph(x=data['x'], a= sparse_matrix, y=data['y']))\n",
    "                \n",
    "               \n",
    "          \n",
    "          \n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8995cfd-6078-44cd-a043-0fa031ac7001",
   "metadata": {},
   "source": [
    "# Load malware data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988f7ac-48f1-4895-bfaf-ac64b1828be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  'data/graph_features/big-15/cfg_embeddings/cfg_embeddings'\n",
    "label_path = \"data/labels/new_gmm_labels_id.csv\"\n",
    "\n",
    "malware = GraphData_Malware_New_Label(path, label_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e46a5cd4-ad81-4c4c-a317-4b0e13503b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 56.23191111940995, 1: 29.931845766034918, 2: 13.42545047147792, 3: 0.41079264307721036}\n"
     ]
    }
   ],
   "source": [
    "label = obtain_labels(malware)\n",
    "uniques, counts = np.unique(label, return_counts=True)\n",
    "\n",
    "percentages = dict(zip(uniques, counts * 100 / len(label)))\n",
    "\n",
    "print(percentages)\n",
    "\n",
    "\n",
    "# 1 Ramnit \t1541 \tWorm 1\n",
    "# 2 Lollipop \t2478 \tAdware \n",
    "# 3 Kelihos_ver3 \t2942 \tBackdoor\n",
    "# 4 Vundo \t475 \tTrojan\n",
    "# 5 Simda \t42 \tBackdoor\n",
    "# 6 Tracur \t751 \tTrojan Downloader\n",
    "# 7 Kelihos_ver1 \t398 \tBackdoor\n",
    "# 8 Obfuscator.ACY \t1228 \tObfuscated malware\n",
    "# 9 Gatak \t1013 \tBackdoor\n",
    "# 0 Normal \t6392 \t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680be95-a01b-4361-a157-cd9d1af8d3b4",
   "metadata": {},
   "source": [
    "# Load normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44f9924f-4a2b-4531-839f-7323857f86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  'data/graph_features/binary_corp/cfg_embeddings'\n",
    "\n",
    "normal = GraphData(path)\n",
    "normal = convert_label_integer(normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9e30f-ff43-403b-91a4-d693892f2b47",
   "metadata": {},
   "source": [
    "# Load normal data windows-PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d61d2b2e-3b8e-4673-aa93-a1935b1330ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  'data/graph_features/windows_PE/cfg_embeddings'\n",
    "\n",
    "normal_PE = GraphData(path)\n",
    "normal_PE = convert_label_integer(normal_PE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dc253c-e439-489a-a99a-4a2bbd6733ec",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "582fa21c-eb43-42ea-96af-9d6068867c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "# Build model\n",
    "################################################################################\n",
    "class GIN0(Model):\n",
    "    def __init__(self, channels, n_layers):\n",
    "        super().__init__()\n",
    "        self.conv1 = GINConv(channels, epsilon=0, mlp_hidden=[channels, channels])\n",
    "        self.convs = []\n",
    "        for _ in range(1, n_layers):\n",
    "            self.convs.append(\n",
    "                GINConv(channels, epsilon=0, mlp_hidden=[channels, channels])\n",
    "            )\n",
    "        self.pool = GlobalAvgPool()\n",
    "        self.dense1 = Dense(channels, activation=\"relu\")\n",
    "      \n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, i = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        for conv in self.convs:\n",
    "            x = conv([x, a])\n",
    "        x = self.pool([x, i])\n",
    "        x = self.dense1(x)\n",
    "      \n",
    "        return x\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0848e99a-30da-4ad5-84d3-99ac0721d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN_GIN(object):\n",
    "    def __init__(self,loader_source_train, \n",
    "                 loader_target_train,\n",
    "                 loader_target_test, GIN, n_classes,\n",
    "                 epochs=90):\n",
    "\n",
    "        #source train and test dataset\n",
    "        self.loader_source_tr = loader_source_train\n",
    "        #self.loader_source_te= loader_source_test\n",
    "        \n",
    "        \n",
    "        # Target train and test dataset\n",
    "        \n",
    "        self.loader_target_tr = loader_target_train\n",
    "        self.loader_target_te= loader_target_test\n",
    "\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "    \n",
    "        \n",
    "        #Latent dim for AE/VAE\n",
    "        self.latent_dim = 256 #25\n",
    "        \n",
    "        \n",
    "        self.generator = GIN \n",
    "        self.epochs = epochs \n",
    "        \n",
    "       \n",
    "        \n",
    "        #Classifier\n",
    "        \n",
    "        class_input = Input(shape=(256,))\n",
    "        #x = latent(class_input)\n",
    "        x= Dense(256, activation = \"relu\")(class_input)\n",
    "        class_output = Dense(self.n_classes, activation = \"softmax\")(x)\n",
    "        \n",
    "        self.classifier = Model(class_input, class_output, name=\"classifier\")\n",
    "        \n",
    "        #Discriminator\n",
    "        \n",
    "        disc_input = Input(shape=(256,))\n",
    "        #x = latent(disc_input)\n",
    "        x = Dense(256, activation = \"relu\")(disc_input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dense(256, activation = \"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        disc_output = Dense(2, activation = \"softmax\")(x)\n",
    "        \n",
    "        self.discriminator = Model(disc_input, disc_output, name=\"discriminator\")\n",
    "\n",
    "        \n",
    "        \n",
    "      \n",
    "        self.loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "        \n",
    "        self.lr = 0.001 \n",
    "        self.momentum = 0.9\n",
    "        self.alpha = 0.0002\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.task_optimizer=  Adam(1e-3)\n",
    "        self.gen_optimizer = Adam(1e-3)\n",
    "        self.disc_optimizer = Adam(1e-3)\n",
    "        \n",
    "        self.train_task_loss = tf.keras.metrics.Mean()\n",
    "        self.train_disc_loss = tf.keras.metrics.Mean()\n",
    "        self.train_gen_loss = tf.keras.metrics.Mean()\n",
    "        self.train_task_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "        self.train_target_task_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "\n",
    "        \n",
    "        self.test_target_task_loss = tf.keras.metrics.Mean()\n",
    "        #self.test_task_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "        self.test_target_task_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "        \n",
    "        #self.test_target_f1_score =  tf.keras.metrics.F1Score(average=\"macro\")\n",
    "\n",
    "\n",
    "        \n",
    "        self.batch_size = 16\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def train_batch(self, x_source_train, y_source_train, x_target_train, y_target_train, epoch):\n",
    "        \n",
    "        \n",
    "        source = np.tile([1,0], (y_source_train.shape[0], 1))\n",
    "        target = np.tile([0,1], (y_target_train.shape[0], 1))\n",
    "        \n",
    "        target_fake = np.tile([0,1], (y_source_train.shape[0], 1))\n",
    "        source_fake = np.tile([1,0], (y_target_train.shape[0], 1))\n",
    "        \n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            y_domain_pred_source = self.discriminator(self.generator(x_source_train, training=True), training=True)\n",
    "            y_domain_pred_target = self.discriminator(self.generator(x_target_train, training=True), training=True)\n",
    "            \n",
    "            disc_loss = self.loss(source, y_domain_pred_source) +  self.loss(target, y_domain_pred_target)  \n",
    "            \n",
    "        disc_grad = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)  \n",
    "        self.disc_optimizer.apply_gradients(zip(disc_grad, self.discriminator.trainable_variables))\n",
    "        self.train_disc_loss(disc_loss)\n",
    "\n",
    "        with tf.GradientTape() as task_tape, tf.GradientTape() as gen_tape:\n",
    "            \n",
    "            #Forward pass\n",
    "            y_class_pred_source = self.classifier(self.generator(x_source_train, training=True), training=True)\n",
    "            y_class_pred_target = self.classifier(self.generator(x_target_train, training=True), training=True)\n",
    "            y_domain_pred_source = self.discriminator(self.generator(x_source_train, training=True), training=True)\n",
    "            y_domain_pred_target = self.discriminator(self.generator(x_target_train, training=True), training=True)\n",
    "            \n",
    "            \n",
    "            task_loss = self.loss(y_target_train, y_class_pred_target) + 0.1*self.loss(y_source_train, y_class_pred_source)  \n",
    "            adv_loss = self.loss(target_fake, y_domain_pred_source) +  self.loss(source_fake, y_domain_pred_target)   \n",
    "            gen_loss = task_loss +  adv_loss*0.1\n",
    "            \n",
    "            #lp_grad = tape.gradient(lp_loss, self.predict_label.trainable_variables)\n",
    "        \n",
    "         # Compute gradients   \n",
    "        task_grad = task_tape.gradient(task_loss, self.classifier.trainable_variables)\n",
    "        gen_grad = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "       \n",
    "        \n",
    "        # Update weights \n",
    "        self.task_optimizer.apply_gradients(zip(task_grad, self.classifier.trainable_variables))\n",
    "        self.gen_optimizer.apply_gradients(zip(gen_grad, self.generator.trainable_variables)) \n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "        self.train_task_loss(task_loss)\n",
    "        self.train_task_accuracy(y_source_train, y_class_pred_source)\n",
    "        self.train_target_task_accuracy(y_target_train, y_class_pred_target)\n",
    "        self.train_gen_loss(gen_loss)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def test_batch(self, x_target_test, y_target_test):\n",
    "       \n",
    "        # y_class_pred = self.classifier(self.generator(x_source_test, training=False), training=False)\n",
    "        y_target_class_pred = self.classifier(self.generator(x_target_test, training=False), training=False)\n",
    "        \n",
    "            \n",
    "        #self.test_task_loss.update_state(y_source_test, y_class_pred)\n",
    "        self.test_target_task_loss(y_target_test, y_target_class_pred)\n",
    "        #self.test_task_accuracy.update_state(y_source_test, y_class_pred)\n",
    "        self.test_target_task_accuracy(y_target_test, y_target_class_pred)\n",
    "        #self.test_target_f1_score.update_state(y_target_test, y_target_class_pred)\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        return #f1\n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        output = []\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "        step = 0\n",
    "        while step < loader.steps_per_epoch:\n",
    "            step += 1\n",
    "            inputs, target = loader.__next__()\n",
    "            pred = self.classifier(self.generator(inputs, training=False), training=False)\n",
    "            outs = (\n",
    "                loss_fn(target, pred),\n",
    "                tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "                len(target),  # Keep track of batch size\n",
    "            )          \n",
    "            output.append(outs)\n",
    "            if step == loader.steps_per_epoch:\n",
    "                output = np.array(output)\n",
    "                return np.average(output[:, :-1], 0, weights=output[:, -1])\n",
    "\n",
    "      \n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "    def log_train(self):\n",
    "        \n",
    "        \n",
    "        log_format = 'C_loss train: {:.4f}, Acc train source: {:.2f} , Acc train target: {:.2f}\\n'+'D_loss train: {:.4f}, G_loss train: {:.4f}'\n",
    "\n",
    "        message = log_format.format(\n",
    "                 self.train_task_loss.result(),\n",
    "                 self.train_task_accuracy.result()*100,\n",
    "                 self.train_target_task_accuracy.result()*100,\n",
    "                 self.train_disc_loss.result(),\n",
    "                 self.train_gen_loss.result())\n",
    "        \n",
    "\n",
    "        self.reset_metrics('train')\n",
    "        #self.reset_metrics('test')\n",
    "\n",
    "\n",
    "        return message \n",
    "    \n",
    "    def log_test(self):\n",
    "        \n",
    "        \n",
    "        log_format = \"C_loss test target: {:.4f}, Acc test target: {:.2f}\"\n",
    "\n",
    "        message = log_format.format(\n",
    "                 #self.test_task_loss.result(),\n",
    "                 #self.test_task_accuracy.result()*100,\n",
    "                 self.test_target_task_loss.result(),\n",
    "                 self.test_target_task_accuracy.result()*100)\n",
    "                 #self.test_target_f1_score.result()*100)\n",
    "        \n",
    "\n",
    "        #self.reset_metrics('train')\n",
    "        self.reset_metrics('test')\n",
    "\n",
    "\n",
    "        return message \n",
    "    \n",
    "    def reset_metrics(self, target):\n",
    "\n",
    "        if target == 'train':\n",
    "            self.train_task_loss.reset_states()\n",
    "            self.train_task_accuracy.reset_states()\n",
    "            self.train_disc_loss.reset_states()\n",
    "            self.train_gen_loss.reset_states()\n",
    "            \n",
    "        #self.reset_metrics('test')\n",
    "        \n",
    "        \n",
    "        if target == 'test':\n",
    "            self.test_target_task_loss.reset_states()\n",
    "            self.test_target_task_accuracy.reset_states()\n",
    "        \n",
    "\n",
    "        return \n",
    "    \n",
    "    def train(self):\n",
    "        epoch = step = 0\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        for (source_batch, source_labels), (target_batch, target_labels) in zip(self.loader_source_tr, self.loader_target_tr):\n",
    "            step +=1 \n",
    "            self.train_batch(source_batch, source_labels, target_batch, target_labels, epoch)\n",
    "            if step == min(self.loader_source_tr.steps_per_epoch, self.loader_target_tr.steps_per_epoch):\n",
    "                step = 0\n",
    "                epoch +=1  \n",
    "                if epoch % 10 ==0:\n",
    "                    print('Epoch: {}'.format(epoch))\n",
    "                    print(self.log_train())                 \n",
    "                    results_te = self.evaluate(self.loader_target_te)\n",
    "                    print(\"Test results - Loss: {:.3f} - Acc: {:.3f}\".format(*results_te))\n",
    "                    \n",
    "\n",
    "        return self.generator, self.classifier\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9490dad4-fa4e-4707-9d38-bd903b7323b8",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf4346-53eb-491a-9225-8df0316b9fc3",
   "metadata": {},
   "source": [
    "## Set Cluster 0 as the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a9da35c-d406-438b-802b-165aae547ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target malware size: 6023\n",
      "Target normal size: 827\n",
      "Source malware size: 4688\n",
      "Source normal size: 6354\n"
     ]
    }
   ],
   "source": [
    "# cluster 0 labels is 0\n",
    "target_malware, source_malware= filter_label(malware, [0])\n",
    "source_normal = normal \n",
    "target_normal = normal_PE\n",
    "\n",
    "\n",
    "# convert it to binary labels\n",
    "target_malware = binary_label(target_malware, True)\n",
    "source_malware  = binary_label(source_malware, True)\n",
    "target_normal = binary_label(target_normal, False)\n",
    "source_normal = binary_label(source_normal, False)\n",
    "\n",
    "print(\"Target malware size: {}\".format(len(target_malware)))\n",
    "print(\"Target normal size: {}\".format(len(target_normal)))\n",
    "print(\"Source malware size: {}\".format(len(source_malware)))\n",
    "print(\"Source normal size: {}\".format(len(source_normal)))\n",
    "           \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69fc78af-16be-44cd-aac3-73286f16863a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3011\n",
      "3012\n",
      "413\n",
      "414\n"
     ]
    }
   ],
   "source": [
    "#split the benign/malware data into train and test for target dataset\n",
    "target_malware_train, target_malware_test = train_test_split(target_malware, 0.5)\n",
    "print(len(target_malware_train))\n",
    "print(len(target_malware_test))\n",
    "\n",
    "target_normal_train, target_normal_test = train_test_split(target_normal, 0.5)\n",
    "print(len(target_normal_train))\n",
    "print(len(target_normal_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e72bb581-91e8-44d7-a275-9781b01262ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target train dataset size: 3424\n",
      "Target test dataset size: 3426\n",
      "Source train dataset size: 8281\n",
      "Source test dataset size: 2761\n"
     ]
    }
   ],
   "source": [
    "# combine the malware and benign data for the source dataset\n",
    "source = merge_dataset(source_normal, source_malware)\n",
    "\n",
    "# source dataset train and test split\n",
    "source_train, source_test = train_test_split(source, 0.75)\n",
    "\n",
    "# we do the same for the target dataset\n",
    "target_train = merge_dataset(target_normal_train, target_malware_train)\n",
    "target_test = merge_dataset(target_normal_test, target_malware_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Target train dataset size: {}\".format(len(target_train)))\n",
    "print(\"Target test dataset size: {}\".format(len(target_test)))\n",
    "print(\"Source train dataset size: {}\".format(len(source_train)))\n",
    "print(\"Source test dataset size: {}\".format((len(source_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65c2b23d-16ba-4ca3-ab13-7319c01784c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Sample size 20-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.1880, Acc train source: 68.44 , Acc train target: 96.50\n",
      "D_loss train: 1.6601, G_loss train: 0.3629\n",
      "Test results - Loss: 0.230 - Acc: 0.900\n",
      "Epoch: 20\n",
      "C_loss train: 0.0918, Acc train source: 81.88 , Acc train target: 97.25\n",
      "D_loss train: 1.4358, G_loss train: 0.2599\n",
      "Test results - Loss: 0.091 - Acc: 0.971\n",
      "Done. Test f1: 0.9658036080910254\n",
      "--------------------------Sample size 50-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.1766, Acc train source: 80.47 , Acc train target: 97.60\n",
      "D_loss train: 1.5552, G_loss train: 0.3503\n",
      "Test results - Loss: 0.100 - Acc: 0.952\n",
      "Epoch: 20\n",
      "C_loss train: 0.1110, Acc train source: 92.34 , Acc train target: 97.60\n",
      "D_loss train: 1.3240, G_loss train: 0.2927\n",
      "Test results - Loss: 0.184 - Acc: 0.952\n",
      "Done. Test f1: 0.941630384651531\n",
      "--------------------------Sample size 100-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.1614, Acc train source: 80.09 , Acc train target: 96.40\n",
      "D_loss train: 1.4800, G_loss train: 0.3303\n",
      "Test results - Loss: 0.189 - Acc: 0.917\n",
      "Epoch: 20\n",
      "C_loss train: 0.0796, Acc train source: 89.29 , Acc train target: 97.55\n",
      "D_loss train: 1.2942, G_loss train: 0.2543\n",
      "Test results - Loss: 0.065 - Acc: 0.982\n",
      "Done. Test f1: 0.9793215540694571\n",
      "--------------------------Sample size 200-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.1122, Acc train source: 87.69 , Acc train target: 97.15\n",
      "D_loss train: 1.4060, G_loss train: 0.2774\n",
      "Test results - Loss: 0.136 - Acc: 0.948\n",
      "Epoch: 20\n",
      "C_loss train: 0.0819, Acc train source: 94.33 , Acc train target: 97.60\n",
      "D_loss train: 1.2444, G_loss train: 0.2643\n",
      "Test results - Loss: 0.115 - Acc: 0.935\n",
      "Done. Test f1: 0.9179798699920966\n",
      "--------------------------Sample size 300-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.0940, Acc train source: 87.73 , Acc train target: 98.03\n",
      "D_loss train: 1.3507, G_loss train: 0.2627\n",
      "Test results - Loss: 0.349 - Acc: 0.899\n",
      "Epoch: 20\n",
      "C_loss train: 0.0537, Acc train source: 90.20 , Acc train target: 98.60\n",
      "D_loss train: 1.2407, G_loss train: 0.2340\n",
      "Test results - Loss: 0.028 - Acc: 0.994\n",
      "Done. Test f1: 0.9941426946881979\n",
      "--------------------------Sample size 500-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.1108, Acc train source: 86.43 , Acc train target: 98.14\n",
      "D_loss train: 1.3400, G_loss train: 0.2823\n",
      "Test results - Loss: 0.106 - Acc: 0.952\n",
      "Epoch: 20\n",
      "C_loss train: 0.0495, Acc train source: 93.80 , Acc train target: 98.66\n",
      "D_loss train: 1.1943, G_loss train: 0.2422\n",
      "Test results - Loss: 0.014 - Acc: 0.997\n",
      "Done. Test f1: 0.9977617467835225\n"
     ]
    }
   ],
   "source": [
    "samples = [20, 50, 100, 200, 300,500]\n",
    "\n",
    "channels = 256  # Hidden units\n",
    "layers = 3  # GIN layers\n",
    "# We have limited the number of training epochs to 20 to minimize training time.\n",
    "# You can change the epochs to a lower number (lowest 1) just to test if the code is functional.\n",
    "# However, for better model performance, consider increasing the number of epochs to those specified in the referenced paper.\n",
    "epochs = 20  \n",
    "batch_size = 16  # Batch size\n",
    "n_out = target_train.n_labels\n",
    "\n",
    "for size in samples: \n",
    "    print(\"--------------------------Sample size {}-------------------------\".format(size))\n",
    "   \n",
    "\n",
    "    target_train_select, target_train_re = subsample(target_train, size)\n",
    "\n",
    "    loader_source_tr = DisjointLoader(source_train, batch_size=batch_size, epochs = epochs, shuffle = True)\n",
    "    loader_source_te = DisjointLoader(source_test,  batch_size=batch_size)\n",
    "\n",
    "\n",
    "    loader_target_tr = DisjointLoader(target_train_select, batch_size=batch_size, epochs = epochs, shuffle = True)\n",
    "    loader_target_te = DisjointLoader(target_test, batch_size=batch_size)\n",
    "\n",
    "    # build model\n",
    "    GIN = GIN0(channels, layers)\n",
    "\n",
    "\n",
    "    model =DANN_GIN(loader_source_tr, loader_target_tr, loader_target_te, GIN, n_out, epochs==20) # change the epochs here as well\n",
    "\n",
    "\n",
    "    G, C = model.train()\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    # Evaluate model\n",
    "    ################################################################################\n",
    "    results = []\n",
    "    step = 0\n",
    "\n",
    "    while step < loader_target_te.steps_per_epoch:      \n",
    "        step += 1\n",
    "        inputs, target = loader_target_te.__next__()\n",
    "        pred = C(G(inputs, training=False), training=False)\n",
    "        results.append(\n",
    "            (\n",
    "               f1_score(np.argmax(target, axis=1), np.argmax(pred, axis=1), average='weighted')\n",
    "            )\n",
    "        )\n",
    "    print(\"Done. Test f1: {}\".format(np.mean(results, 0)))\n",
    "\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f8a39-ca97-46ee-bafa-75629b53530a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8d72947-0725-4124-9aed-d388799da985",
   "metadata": {},
   "source": [
    "## Set Cluster 1 as the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60917760-ea48-4da4-97c8-0bbeb8601cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target malware size: 3206\n",
      "Target normal size: 827\n",
      "Source malware size: 7505\n",
      "Source normal size: 6354\n"
     ]
    }
   ],
   "source": [
    "# cluster 1 labels is 1\n",
    "target_malware, source_malware= filter_label(malware, [1])\n",
    "source_normal = normal \n",
    "target_normal = normal_PE\n",
    "\n",
    "\n",
    "# convert it to binary labels\n",
    "target_malware = binary_label(target_malware, True)\n",
    "source_malware  = binary_label(source_malware, True)\n",
    "target_normal = binary_label(target_normal, False)\n",
    "source_normal = binary_label(source_normal, False)\n",
    "\n",
    "print(\"Target malware size: {}\".format(len(target_malware)))\n",
    "print(\"Target normal size: {}\".format(len(target_normal)))\n",
    "print(\"Source malware size: {}\".format(len(source_malware)))\n",
    "print(\"Source normal size: {}\".format(len(source_normal)))\n",
    "           \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c437212-3c12-41e3-84b4-5c5b396485d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1603\n",
      "1603\n",
      "413\n",
      "414\n"
     ]
    }
   ],
   "source": [
    "#split the benign/malware data into train and test for target dataset\n",
    "target_malware_train, target_malware_test = train_test_split(target_malware, 0.5)\n",
    "print(len(target_malware_train))\n",
    "print(len(target_malware_test))\n",
    "\n",
    "target_normal_train, target_normal_test = train_test_split(target_normal, 0.5)\n",
    "print(len(target_normal_train))\n",
    "print(len(target_normal_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b03e260f-bbaf-49dc-b5e5-2c4446480dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target train dataset size: 2016\n",
      "Target test dataset size: 2017\n",
      "Source train dataset size: 10394\n",
      "Source test dataset size: 3465\n"
     ]
    }
   ],
   "source": [
    "# combine the malware and benign data for source dataset\n",
    "source = merge_dataset(source_normal, source_malware)\n",
    "\n",
    "# source dataset train and test split\n",
    "source_train, source_test = train_test_split(source, 0.75)\n",
    "\n",
    "# we do the same for the target dataset\n",
    "target_train = merge_dataset(target_normal_train, target_malware_train)\n",
    "target_test = merge_dataset(target_normal_test, target_malware_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Target train dataset size: {}\".format(len(target_train)))\n",
    "print(\"Target test dataset size: {}\".format(len(target_test)))\n",
    "print(\"Source train dataset size: {}\".format(len(source_train)))\n",
    "print(\"Source test dataset size: {}\".format((len(source_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b6e20e6-a765-4862-8586-170a3a762979",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Sample size 20-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.1697, Acc train source: 84.06 , Acc train target: 96.50\n",
      "D_loss train: 1.5975, G_loss train: 0.3371\n",
      "Test results - Loss: 1.027 - Acc: 0.840\n",
      "Epoch: 20\n",
      "C_loss train: 0.0633, Acc train source: 95.62 , Acc train target: 98.25\n",
      "D_loss train: 1.4603, G_loss train: 0.2309\n",
      "Test results - Loss: 0.495 - Acc: 0.926\n",
      "Done. Test f1: 0.9072291570157861\n",
      "--------------------------Sample size 50-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.2067, Acc train source: 89.69 , Acc train target: 96.20\n",
      "D_loss train: 1.4950, G_loss train: 0.3810\n",
      "Test results - Loss: 0.634 - Acc: 0.882\n",
      "Epoch: 20\n",
      "C_loss train: 0.2108, Acc train source: 88.91 , Acc train target: 96.80\n",
      "D_loss train: 1.3722, G_loss train: 0.3815\n",
      "Test results - Loss: 0.097 - Acc: 0.957\n",
      "Done. Test f1: 0.9517511006059881\n",
      "--------------------------Sample size 100-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.1744, Acc train source: 90.09 , Acc train target: 95.60\n",
      "D_loss train: 1.5296, G_loss train: 0.3384\n",
      "Test results - Loss: 0.009 - Acc: 0.997\n",
      "Epoch: 20\n",
      "C_loss train: 0.1663, Acc train source: 89.29 , Acc train target: 96.50\n",
      "D_loss train: 1.3890, G_loss train: 0.3267\n",
      "Test results - Loss: 0.031 - Acc: 0.998\n",
      "Done. Test f1: 0.9982489870418162\n",
      "--------------------------Sample size 200-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.1289, Acc train source: 89.13 , Acc train target: 96.95\n",
      "D_loss train: 1.4176, G_loss train: 0.2988\n",
      "Test results - Loss: 0.088 - Acc: 0.964\n",
      "Epoch: 20\n",
      "C_loss train: 0.0402, Acc train source: 93.75 , Acc train target: 98.25\n",
      "D_loss train: 1.2610, G_loss train: 0.2206\n",
      "Test results - Loss: 0.217 - Acc: 0.894\n",
      "Done. Test f1: 0.8722940049412498\n",
      "--------------------------Sample size 300-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.0937, Acc train source: 92.50 , Acc train target: 97.17\n",
      "D_loss train: 1.3901, G_loss train: 0.2648\n",
      "Test results - Loss: 0.004 - Acc: 1.000\n",
      "Epoch: 20\n",
      "C_loss train: 0.0503, Acc train source: 95.07 , Acc train target: 98.07\n",
      "D_loss train: 1.3180, G_loss train: 0.2170\n",
      "Test results - Loss: 0.027 - Acc: 0.993\n",
      "Done. Test f1: 0.9923935299506125\n",
      "--------------------------Sample size 500-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.0928, Acc train source: 94.34 , Acc train target: 97.26\n",
      "D_loss train: 1.3478, G_loss train: 0.2583\n",
      "Test results - Loss: 0.254 - Acc: 0.941\n",
      "Epoch: 20\n",
      "C_loss train: 0.0409, Acc train source: 97.38 , Acc train target: 98.10\n",
      "D_loss train: 1.3140, G_loss train: 0.2045\n",
      "Test results - Loss: 0.035 - Acc: 0.999\n",
      "Done. Test f1: 0.9992478962690718\n"
     ]
    }
   ],
   "source": [
    "samples = [20, 50, 100, 200, 300, 500]\n",
    "\n",
    "channels = 256  # Hidden units\n",
    "layers = 3  # GIN layers\n",
    "\n",
    "# We have limited the number of training epochs to 20 to minimize training time.\n",
    "# You can change the epochs to a lower number (lowest 1) just to test if the code is functional.\n",
    "# However, for better model performance, consider increasing the number of epochs to those specified in the referenced paper.\n",
    "epochs = 20  \n",
    "batch_size = 16  # Batch size\n",
    "n_out = target_train.n_labels\n",
    "\n",
    "for size in samples: \n",
    "    print(\"--------------------------Sample size {}-------------------------\".format(size))\n",
    "\n",
    "\n",
    "    target_train_select, target_train_re = subsample(target_train, size)\n",
    "\n",
    "    loader_source_tr = DisjointLoader(source_train, batch_size=batch_size, epochs = epochs, shuffle = True)\n",
    "    loader_source_te = DisjointLoader(source_test,  batch_size=batch_size)\n",
    "\n",
    "\n",
    "    loader_target_tr = DisjointLoader(target_train_select, batch_size=batch_size, epochs = epochs, shuffle = True)\n",
    "    loader_target_te = DisjointLoader(target_test, batch_size=batch_size)\n",
    "\n",
    "    # build model\n",
    "    GIN = GIN0(channels, layers)\n",
    "\n",
    "\n",
    "    model =DANN_GIN(loader_source_tr, loader_target_tr, loader_target_te, GIN, n_out, epochs==20) # change the epochs here as well\n",
    "\n",
    "\n",
    "    G, C = model.train()\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    # Evaluate model\n",
    "    ################################################################################\n",
    "    results = []\n",
    "    step = 0\n",
    "\n",
    "    while step < loader_target_te.steps_per_epoch:      \n",
    "        step += 1\n",
    "        inputs, target = loader_target_te.__next__()\n",
    "        pred = C(G(inputs, training=False), training=False)\n",
    "        results.append(\n",
    "            (\n",
    "               f1_score(np.argmax(target, axis=1), np.argmax(pred, axis=1), average='weighted')\n",
    "            )\n",
    "        )\n",
    "    print(\"Done. Test f1: {}\".format(np.mean(results, 0)))\n",
    "\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f670a-5a90-4b0d-8742-e5b73af959db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6137f79c-f9ae-4b0f-a77e-6763c66fba8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set Cluster 2 as the target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f524f693-3454-425b-a9c7-4831c8547c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target malware size: 1438\n",
      "Target normal size: 827\n",
      "Source malware size: 9273\n",
      "Source normal size: 6354\n"
     ]
    }
   ],
   "source": [
    "# cluster 2 labels is 2\n",
    "target_malware, source_malware= filter_label(malware, [2])\n",
    "source_normal = normal \n",
    "target_normal = normal_PE\n",
    "\n",
    "# convert it to binary labels\n",
    "target_malware = binary_label(target_malware, True)\n",
    "source_malware  = binary_label(source_malware, True)\n",
    "target_normal = binary_label(target_normal, False)\n",
    "source_normal = binary_label(source_normal, False)\n",
    "\n",
    "print(\"Target malware size: {}\".format(len(target_malware)))\n",
    "print(\"Target normal size: {}\".format(len(target_normal)))\n",
    "print(\"Source malware size: {}\".format(len(source_malware)))\n",
    "print(\"Source normal size: {}\".format(len(source_normal)))\n",
    "           \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62702cd1-e8fe-4fb7-a17e-18315270b1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719\n",
      "719\n",
      "413\n",
      "414\n"
     ]
    }
   ],
   "source": [
    "#split the benign/malware data into train and test for target dataset\n",
    "\n",
    "target_malware_train, target_malware_test = train_test_split(target_malware, 0.5)\n",
    "print(len(target_malware_train))\n",
    "print(len(target_malware_test))\n",
    "\n",
    "target_normal_train, target_normal_test = train_test_split(target_normal, 0.5)\n",
    "print(len(target_normal_train))\n",
    "print(len(target_normal_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b7b246e-c240-454a-8e13-9cfc9eed1cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target train dataset size: 1132\n",
      "Target test dataset size: 1133\n",
      "Source train dataset size: 11720\n",
      "Source test dataset size: 3907\n"
     ]
    }
   ],
   "source": [
    "# combine the malware and benign data for source dataset\n",
    "source = merge_dataset(source_normal, source_malware)\n",
    "\n",
    "# source dataset train and test split\n",
    "source_train, source_test = train_test_split(source, 0.75)\n",
    "# we do the same for the target dataset\n",
    "target_train = merge_dataset(target_normal_train, target_malware_train)\n",
    "target_test = merge_dataset(target_normal_test, target_malware_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Target train dataset size: {}\".format(len(target_train)))\n",
    "print(\"Target test dataset size: {}\".format(len(target_test)))\n",
    "print(\"Source train dataset size: {}\".format(len(source_train)))\n",
    "print(\"Source test dataset size: {}\".format((len(source_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40a4f508-eebc-4bff-90d6-81e2100986f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Sample size 20-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.2090, Acc train source: 66.88 , Acc train target: 94.00\n",
      "D_loss train: 1.5965, G_loss train: 0.3840\n",
      "Test results - Loss: 0.068 - Acc: 0.978\n",
      "Epoch: 20\n",
      "C_loss train: 0.1676, Acc train source: 87.81 , Acc train target: 96.00\n",
      "D_loss train: 1.3657, G_loss train: 0.3512\n",
      "Test results - Loss: 0.088 - Acc: 0.974\n",
      "Done. Test f1: 0.9729685400761007\n",
      "--------------------------Sample size 50-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.3155, Acc train source: 91.88 , Acc train target: 94.00\n",
      "D_loss train: 1.5361, G_loss train: 0.4949\n",
      "Test results - Loss: 0.045 - Acc: 0.988\n",
      "Epoch: 20\n",
      "C_loss train: 0.0983, Acc train source: 90.47 , Acc train target: 96.70\n",
      "D_loss train: 1.3337, G_loss train: 0.2762\n",
      "Test results - Loss: 0.008 - Acc: 0.996\n",
      "Done. Test f1: 0.9965716292640755\n",
      "--------------------------Sample size 100-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.1103, Acc train source: 89.46 , Acc train target: 96.20\n",
      "D_loss train: 1.4201, G_loss train: 0.2764\n",
      "Test results - Loss: 0.001 - Acc: 1.000\n",
      "Epoch: 20\n",
      "C_loss train: 0.1991, Acc train source: 94.38 , Acc train target: 95.90\n",
      "D_loss train: 1.3429, G_loss train: 0.3613\n",
      "Test results - Loss: 0.119 - Acc: 0.975\n",
      "Done. Test f1: 0.9744850739572415\n",
      "--------------------------Sample size 200-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.0722, Acc train source: 93.46 , Acc train target: 98.20\n",
      "D_loss train: 1.3887, G_loss train: 0.2515\n",
      "Test results - Loss: 0.013 - Acc: 0.995\n",
      "Epoch: 20\n",
      "C_loss train: 0.0390, Acc train source: 95.87 , Acc train target: 98.72\n",
      "D_loss train: 1.2219, G_loss train: 0.2239\n",
      "Test results - Loss: 0.038 - Acc: 0.989\n",
      "Done. Test f1: 0.9885335270333347\n",
      "--------------------------Sample size 300-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.0544, Acc train source: 94.70 , Acc train target: 98.60\n",
      "D_loss train: 1.3034, G_loss train: 0.2339\n",
      "Test results - Loss: 0.007 - Acc: 0.996\n",
      "Epoch: 20\n",
      "C_loss train: 0.0328, Acc train source: 95.16 , Acc train target: 99.13\n",
      "D_loss train: 1.2849, G_loss train: 0.2103\n",
      "Test results - Loss: 0.016 - Acc: 0.992\n",
      "Done. Test f1: 0.9919905582433381\n",
      "--------------------------Sample size 500-------------------------\n",
      "Epoch: 10\n",
      "C_loss train: 0.0971, Acc train source: 93.26 , Acc train target: 97.92\n",
      "D_loss train: 1.3204, G_loss train: 0.2730\n",
      "Test results - Loss: 0.006 - Acc: 0.998\n",
      "Epoch: 20\n",
      "C_loss train: 0.0629, Acc train source: 95.18 , Acc train target: 98.65\n",
      "D_loss train: 1.2666, G_loss train: 0.2363\n",
      "Test results - Loss: 0.005 - Acc: 0.998\n",
      "Done. Test f1: 0.9981834733094926\n"
     ]
    }
   ],
   "source": [
    "samples = [20, 50, 100, 200, 300, 500]\n",
    "\n",
    "channels = 256  # Hidden units\n",
    "layers = 3  # GIN layers\n",
    "\n",
    "# We have limited the number of training epochs to 20 to minimize training time.\n",
    "# You can change the epochs to a lower number (lowest 1) just to test if the code is functional.\n",
    "# However, for better model performance, consider increasing the number of epochs to those specified in the referenced paper.\n",
    "epochs = 20  \n",
    "batch_size = 16  # Batch size\n",
    "n_out = target_train.n_labels\n",
    "\n",
    "for size in samples: \n",
    "    print(\"--------------------------Sample size {}-------------------------\".format(size))\n",
    "   \n",
    "\n",
    "    target_train_select, target_train_re = subsample(target_train, size)\n",
    "\n",
    "    loader_source_tr = DisjointLoader(source_train, batch_size=batch_size, epochs = epochs, shuffle = True)\n",
    "    loader_source_te = DisjointLoader(source_test,  batch_size=batch_size)\n",
    "\n",
    "\n",
    "    loader_target_tr = DisjointLoader(target_train_select, batch_size=batch_size, epochs = epochs, shuffle = True)\n",
    "    loader_target_te = DisjointLoader(target_test, batch_size=batch_size)\n",
    "\n",
    "    # build model\n",
    "    GIN = GIN0(channels, layers)\n",
    "\n",
    "\n",
    "    model =DANN_GIN(loader_source_tr, loader_target_tr, loader_target_te, GIN, n_out, epochs==20) # change the epochs here as well\n",
    "\n",
    "\n",
    "    G, C = model.train()\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    # Evaluate model\n",
    "    ################################################################################\n",
    "    results = []\n",
    "    step = 0\n",
    "\n",
    "    while step < loader_target_te.steps_per_epoch:      \n",
    "        step += 1\n",
    "        inputs, target = loader_target_te.__next__()\n",
    "        pred = C(G(inputs, training=False), training=False)\n",
    "        results.append(\n",
    "            (\n",
    "               f1_score(np.argmax(target, axis=1), np.argmax(pred, axis=1), average='weighted')\n",
    "            )\n",
    "        )\n",
    "    print(\"Done. Test f1: {}\".format(np.mean(results, 0)))\n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8daf6e-f7d3-48ca-87a5-06ecefb559f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ipykernel",
   "language": "python",
   "name": "ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
